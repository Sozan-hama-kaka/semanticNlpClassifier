titles,summaries,terms
Prime Sample Attention in Object Detection,"It is a common paradigm in object detection frameworks to treat all samples
equally and target at maximizing the performance on average. In this work, we
revisit this paradigm through a careful study on how different samples
contribute to the overall performance measured in terms of mAP. Our study
suggests that the samples in each mini-batch are neither independent nor
equally important, and therefore a better classifier on average does not
necessarily mean higher mAP. Motivated by this study, we propose the notion of
Prime Samples, those that play a key role in driving the detection performance.
We further develop a simple yet effective sampling and learning strategy called
PrIme Sample Attention (PISA) that directs the focus of the training process
towards such samples. Our experiments demonstrate that it is often more
effective to focus on prime samples than hard samples when training a detector.
Particularly, On the MSCOCO dataset, PISA outperforms the random sampling
baseline and hard mining schemes, e.g., OHEM and Focal Loss, consistently by
around 2% on both single-stage and two-stage detectors, even with a strong
backbone ResNeXt-101.",['Computer Vision and Pattern Recognition']
A Strong Baseline for Weekly Time Series Forecasting,"Many businesses and industries require accurate forecasts for weekly time
series nowadays. The forecasting literature however does not currently provide
easy-to-use, automatic, reproducible and accurate approaches dedicated to this
task. We propose a forecasting method that can be used as a strong baseline in
this domain, leveraging state-of-the-art forecasting techniques, forecast
combination, and global modelling. Our approach uses four base forecasting
models specifically suitable for forecasting weekly data: a global Recurrent
Neural Network model, Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS),
and Dynamic Harmonic Regression ARIMA (DHR-ARIMA). Those are then optimally
combined using a lasso regression stacking approach. We evaluate the
performance of our method against a set of state-of-the-art weekly forecasting
models on six datasets. Across four evaluation metrics, we show that our method
consistently outperforms the benchmark methods by a considerable margin with
statistical significance. In particular, our model can produce the most
accurate forecasts, in terms of mean sMAPE, for the M4 weekly dataset.","['Machine Learning', 'Artificial Intelligence', 'Neural and Evolutionary Computing']"
Sequential convolutional network for behavioral pattern extraction in gait recognition,"As a unique and promising biometric, video-based gait recognition has broad
applications. The key step of this methodology is to learn the walking pattern
of individuals, which, however, often suffers challenges to extract the
behavioral feature from a sequence directly. Most existing methods just focus
on either the appearance or the motion pattern. To overcome these limitations,
we propose a sequential convolutional network (SCN) from a novel perspective,
where spatiotemporal features can be learned by a basic convolutional backbone.
In SCN, behavioral information extractors (BIE) are constructed to comprehend
intermediate feature maps in time series through motion templates where the
relationship between frames can be analyzed, thereby distilling the information
of the walking pattern. Furthermore, a multi-frame aggregator in SCN performs
feature integration on a sequence whose length is uncertain, via a mobile 3D
convolutional layer. To demonstrate the effectiveness, experiments have been
conducted on two popular public benchmarks, CASIA-B and OU-MVLP, and our
approach is demonstrated superior performance, comparing with the state-of-art
methods.",['Computer Vision and Pattern Recognition']
Multivariate Time Series Imputation by Graph Neural Networks,"Dealing with missing values and incomplete time series is a labor-intensive
and time-consuming inevitable task when handling data coming from real-world
applications. Effective spatio-temporal representations would allow imputation
methods to reconstruct missing temporal data by exploiting information coming
from sensors at different locations. However, standard methods fall short in
capturing the nonlinear time and space dependencies existing within networks of
interconnected sensors and do not take full advantage of the available - and
often strong - relational information. Notably, most of state-of-the-art
imputation methods based on deep learning do not explicitly model relational
aspects and, in any case, do not exploit processing frameworks able to
adequately represent structured spatio-temporal data. Conversely, graph neural
networks have recently surged in popularity as both expressive and scalable
tools for processing sequential data with relational inductive biases. In this
work, we present the first assessment of graph neural networks in the context
of multivariate time series imputation. In particular, we introduce a novel
graph neural network architecture, named GRIL, which aims at reconstructing
missing data in the different channels of a multivariate time series by
learning spatial-temporal representations through message passing. Preliminary
empirical results show that our model outperforms state-of-the-art methods in
the imputation task on relevant benchmarks with mean absolute error
improvements often higher than 20%.","['Machine Learning', 'Artificial Intelligence']"
Exploiting the potential of unlabeled endoscopic video data with self-supervised learning,"Surgical data science is a new research field that aims to observe all
aspects of the patient treatment process in order to provide the right
assistance at the right time. Due to the breakthrough successes of deep
learning-based solutions for automatic image annotation, the availability of
reference annotations for algorithm training is becoming a major bottleneck in
the field. The purpose of this paper was to investigate the concept of
self-supervised learning to address this issue.
  Our approach is guided by the hypothesis that unlabeled video data can be
used to learn a representation of the target domain that boosts the performance
of state-of-the-art machine learning algorithms when used for pre-training.
Core of the method is an auxiliary task based on raw endoscopic video data of
the target domain that is used to initialize the convolutional neural network
(CNN) for the target task. In this paper, we propose the re-colorization of
medical images with a generative adversarial network (GAN)-based architecture
as auxiliary task. A variant of the method involves a second pre-training step
based on labeled data for the target task from a related domain. We validate
both variants using medical instrument segmentation as target task.
  The proposed approach can be used to radically reduce the manual annotation
effort involved in training CNNs. Compared to the baseline approach of
generating annotated data from scratch, our method decreases exploratively the
number of labeled images by up to 75% without sacrificing performance. Our
method also outperforms alternative methods for CNN pre-training, such as
pre-training on publicly available non-medical or medical data using the target
task (in this instance: segmentation).
  As it makes efficient use of available (non-)public and (un-)labeled data,
the approach has the potential to become a valuable tool for CNN
(pre-)training.",['Computer Vision and Pattern Recognition']
Graph Regularized Nonnegative Tensor Ring Decomposition for Multiway Representation Learning,"Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank
nature of multiway data and has demonstrated great potential in a variety of
important applications. In this paper, nonnegative tensor ring (NTR)
decomposition and graph regularized NTR (GNTR) decomposition are proposed,
where the former equips TR decomposition with local feature extraction by
imposing nonnegativity on the core tensors and the latter is additionally able
to capture manifold geometry information of tensor data, both significantly
extend the applications of TR decomposition for nonnegative multiway
representation learning. Accelerated proximal gradient based methods are
derived for NTR and GNTR. The experimental result demonstrate that the proposed
algorithms can extract parts-based basis with rich colors and rich lines from
tensor objects that provide more interpretable and meaningful representation,
and hence yield better performance than the state-of-the-art tensor based
methods in clustering and classification tasks.",['Computer Vision and Pattern Recognition']
Designing A Composite Dictionary Adaptively From Joint Examples,"We study the complementary behaviors of external and internal examples in
image restoration, and are motivated to formulate a composite dictionary design
framework. The composite dictionary consists of the global part learned from
external examples, and the sample-specific part learned from internal examples.
The dictionary atoms in both parts are further adaptively weighted to emphasize
their model statistics. Experiments demonstrate that the joint utilization of
external and internal examples leads to substantial improvements, with
successful applications in image denoising and super resolution.",['Computer Vision and Pattern Recognition']
Differentiable Scaffolding Tree for Molecular Optimization,"The structural design of functional molecules, also called molecular
optimization, is an essential chemical science and engineering task with
important applications, such as drug discovery. Deep generative models and
combinatorial optimization methods achieve initial success but still struggle
with directly modeling discrete chemical structures and often heavily rely on
brute-force enumeration. The challenge comes from the discrete and
non-differentiable nature of molecule structures. To address this, we propose
differentiable scaffolding tree (DST) that utilizes a learned knowledge network
to convert discrete chemical structures to locally differentiable ones. DST
enables a gradient-based optimization on a chemical graph structure by
back-propagating the derivatives from the target properties through a graph
neural network (GNN). Our empirical studies show the gradient-based molecular
optimizations are both effective and sample efficient. Furthermore, the learned
graph parameters can also provide an explanation that helps domain experts
understand the model output.",['Machine Learning']
Group-CAM: Group Score-Weighted Visual Explanations for Deep Convolutional Networks,"In this paper, we propose an efficient saliency map generation method, called
Group score-weighted Class Activation Mapping (Group-CAM), which adopts the
""split-transform-merge"" strategy to generate saliency maps. Specifically, for
an input image, the class activations are firstly split into groups. In each
group, the sub-activations are summed and de-noised as an initial mask. After
that, the initial masks are transformed with meaningful perturbations and then
applied to preserve sub-pixels of the input (i.e., masked inputs), which are
then fed into the network to calculate the confidence scores. Finally, the
initial masks are weighted summed to form the final saliency map, where the
weights are confidence scores produced by the masked inputs. Group-CAM is
efficient yet effective, which only requires dozens of queries to the network
while producing target-related saliency maps. As a result, Group-CAM can be
served as an effective data augment trick for fine-tuning the networks. We
comprehensively evaluate the performance of Group-CAM on common-used
benchmarks, including deletion and insertion tests on ImageNet-1k, and pointing
game tests on COCO2017. Extensive experimental results demonstrate that
Group-CAM achieves better visual performance than the current state-of-the-art
explanation approaches. The code is available at
https://github.com/wofmanaf/Group-CAM.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
Multi-modal Egocentric Activity Recognition using Audio-Visual Features,"Egocentric activity recognition in first-person videos has an increasing
importance with a variety of applications such as lifelogging, summarization,
assisted-living and activity tracking. Existing methods for this task are based
on interpretation of various sensor information using pre-determined weights
for each feature. In this work, we propose a new framework for egocentric
activity recognition problem based on combining audio-visual features with
multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that
purpose, firstly grid optical-flow, virtual-inertia feature, log-covariance,
cuboid are extracted from the video. The audio signal is characterized using a
""supervector"", obtained based on Gaussian mixture modelling of frame-level
features, followed by a maximum a-posteriori adaptation. Then, the extracted
multi-modal features are adaptively fused by MKL classifiers in which both the
feature and kernel selection/weighing and recognition tasks are performed
together. The proposed framework was evaluated on a number of egocentric
datasets. The results showed that using multi-modal features with MKL
outperforms the existing methods.",['Computer Vision and Pattern Recognition']
labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds,"Within the past decade, the rise of applications based on artificial
intelligence (AI) in general and machine learning (ML) in specific has led to
many significant contributions within different domains. The applications range
from robotics over medical diagnoses up to autonomous driving. However, nearly
all applications rely on trained data. In case this data consists of 3D images,
it is of utmost importance that the labeling is as accurate as possible to
ensure high-quality outcomes of the ML models. Labeling in the 3D space is
mostly manual work performed by expert workers, where they draw 3D bounding
boxes around target objects the ML model should later automatically identify,
e.g., pedestrians for autonomous driving or cancer cells within radiography.
  While a small range of recent 3D labeling tools exist, they all share three
major shortcomings: (i) they are specified for autonomous driving applications,
(ii) they lack convenience and comfort functions, and (iii) they have high
dependencies and little flexibility in data format. Therefore, we propose a
novel labeling tool for 3D object detection in point clouds to address these
shortcomings.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Multi-Resolution Multi-Modal Sensor Fusion For Remote Sensing Data With Label Uncertainty,"In remote sensing, each sensor can provide complementary or reinforcing
information. It is valuable to fuse outputs from multiple sensors to boost
overall performance. Previous supervised fusion methods often require accurate
labels for each pixel in the training data. However, in many remote sensing
applications, pixel-level labels are difficult or infeasible to obtain. In
addition, outputs from multiple sensors often have different resolution or
modalities. For example, rasterized hyperspectral imagery presents data in a
pixel grid while airborne Light Detection and Ranging (LiDAR) generates dense
three-dimensional (3D) point clouds. It is often difficult to directly fuse
such multi-modal, multi-resolution data. To address these challenges, we
present a novel Multiple Instance Multi-Resolution Fusion (MIMRF) framework
that can fuse multi-resolution and multi-modal sensor outputs while learning
from automatically-generated, imprecisely-labeled data. Experiments were
conducted on the MUUFL Gulfport hyperspectral and LiDAR data set and a
remotely-sensed soybean and weed data set. Results show improved, consistent
performance on scene understanding and agricultural applications when compared
to traditional fusion methods.",['Computer Vision and Pattern Recognition']
Cross-Network Learning with Partially Aligned Graph Convolutional Networks,"Graph neural networks have been widely used for learning representations of
nodes for many downstream tasks on graph data. Existing models were designed
for the nodes on a single graph, which would not be able to utilize information
across multiple graphs. The real world does have multiple graphs where the
nodes are often partially aligned. For examples, knowledge graphs share a
number of named entities though they may have different relation schema;
collaboration networks on publications and awarded projects share some
researcher nodes who are authors and investigators, respectively; people use
multiple web services, shopping, tweeting, rating movies, and some may register
the same email account across the platforms. In this paper, I propose partially
aligned graph convolutional networks to learn node representations across the
models. I investigate multiple methods (including model sharing,
regularization, and alignment reconstruction) as well as theoretical analysis
to positively transfer knowledge across the (small) set of partially aligned
nodes. Extensive experiments on real-world knowledge graphs and collaboration
networks show the superior performance of our proposed methods on relation
classification and link prediction.",['Machine Learning']
"RelTransformer: Balancing the Visual Relationship Detection from Local Context, Scene and Memory","Visual relationship recognition (VRR) is a fundamental scene understanding
task. The structure that VRR provides is essential to improve the AI
interpretability in downstream tasks such as image captioning and visual
question answering. Several recent studies showed that the long-tail problem in
VRR is even more critical than that in object recognition due to the
compositional complexity and structure. To overcome this limitation, we propose
a novel transformer-based framework, dubbed as RelTransformer, which performs
relationship prediction using rich semantic features from multiple image
levels. We assume that more abundantcon textual features can generate more
accurate and discriminative relationships, which can be useful when sufficient
training data are lacking. The key feature of our model is its ability to
aggregate three different-level features (local context, scene, and
dataset-level) to compositionally predict the visual relationship. We evaluate
our model on the visual genome and two ""long-tail"" VRR datasets, GQA-LT and
VG8k-LT. Extensive experiments demonstrate that our RelTransformer could
improve over the state-of-the-art baselines on all the datasets. In addition,
our model significantly improves the accuracy of GQA-LT by 27.4% upon the best
baselines on tail-relationship prediction. Our code is available in
https://github.com/Vision-CAIR/RelTransformer.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
Explainable Deep Behavioral Sequence Clustering for Transaction Fraud Detection,"In e-commerce industry, user behavior sequence data has been widely used in
many business units such as search and merchandising to improve their products.
However, it is rarely used in financial services not only due to its 3V
characteristics - i.e. Volume, Velocity and Variety - but also due to its
unstructured nature. In this paper, we propose a Financial Service scenario
Deep learning based Behavior data representation method for Clustering
(FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the
behavior sequence data, we treat click stream data as event sequence, use time
attention based Bi-LSTM to learn the sequence embedding in an unsupervised
fashion, and combine them with intuitive features generated by risk experts to
form a hybrid feature representation. We also propose a GPU powered HDBSCAN
(pHDBSCAN) algorithm, which is an engineering optimization for the original
HDBSCAN algorithm based on FAISS project, so that clustering can be carried out
on hundreds of millions of transactions within a few minutes. The computation
efficiency of the algorithm has increased 500 times compared with the original
implementation, which makes flash fraud pattern detection feasible. Our
experimental results show that the proposed FinDeepBehaviorCluster framework is
able to catch missed fraudulent transactions with considerable business values.
In addition, rule extraction method is applied to extract patterns from risky
clusters using intuitive features, so that narrative descriptions can be
attached to the risky clusters for case investigation, and unknown risk
patterns can be mined for real-time fraud detection. In summary,
FinDeepBehaviorCluster as a complementary risk management strategy to the
existing real-time fraud detection engine, can further increase our fraud
detection and proactive risk defense capabilities.",['Machine Learning']
Hidden Two-Stream Convolutional Networks for Action Recognition,"Analyzing videos of human actions involves understanding the temporal
relationships among video frames. State-of-the-art action recognition
approaches rely on traditional optical flow estimation methods to pre-compute
motion information for CNNs. Such a two-stage approach is computationally
expensive, storage demanding, and not end-to-end trainable. In this paper, we
present a novel CNN architecture that implicitly captures motion information
between adjacent frames. We name our approach hidden two-stream CNNs because it
only takes raw video frames as input and directly predicts action classes
without explicitly computing optical flow. Our end-to-end approach is 10x
faster than its two-stage baseline. Experimental results on four challenging
action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show
that our approach significantly outperforms the previous best real-time
approaches.","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Multimedia']"
Screen Content Image Segmentation Using Sparse-Smooth Decomposition,"Sparse decomposition has been extensively used for different applications
including signal compression and denoising and document analysis. In this
paper, sparse decomposition is used for image segmentation. The proposed
algorithm separates the background and foreground using a sparse-smooth
decomposition technique such that the smooth and sparse components correspond
to the background and foreground respectively. This algorithm is tested on
several test images from HEVC test sequences and is shown to have superior
performance over other methods, such as the hierarchical k-means clustering in
DjVu. This segmentation algorithm can also be used for text extraction, video
compression and medical image segmentation.",['Computer Vision and Pattern Recognition']
Visual Categorization of Objects into Animal and Plant Classes Using Global Shape Descriptors,"How humans can distinguish between general categories of objects? Are the
subcategories of living things visually distinctive? In a number of
semantic-category deficits, patients are good at making broad categorization
but are unable to remember fine and specific details. It has been well accepted
that general information about concepts are more robust to damages related to
semantic memory. Results from patients with semantic memory disorders
demonstrate the loss of ability in subcategory recognition. While bottom-up
feature construction has been studied in detail, little attention has been
served to top-down approach and the type of features that could account for
general categorization. In this paper, we show that broad categories of animal
and plant are visually distinguishable without processing textural information.
To this aim, we utilize shape descriptors with an additional phase of feature
learning. The results are evaluated with both supervised and unsupervised
learning mechanisms. The obtained results demonstrate that global encoding of
visual appearance of objects accounts for high discrimination between animal
and plant object categories.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
"Cats, not CAT scans: a study of dataset similarity in transfer learning for 2D medical image classification","Transfer learning is a commonly used strategy for medical image
classification, especially via pretraining on source data and fine-tuning on
target data. There is currently no consensus on how to choose appropriate
source data, and in the literature we can find both evidence of favoring large
natural image datasets such as ImageNet, and evidence of favoring more
specialized medical datasets. In this paper we perform a systematic study with
nine source datasets with natural or medical images, and three target medical
datasets, all with 2D images. We find that ImageNet is the source leading to
the highest performances, but also that larger datasets are not necessarily
better. We also study different definitions of data similarity. We show that
common intuitions about similarity may be inaccurate, and therefore not
sufficient to predict an appropriate source a priori. Finally, we discuss
several steps needed for further research in this field, especially with regard
to other types (for example 3D) medical images. Our experiments and pretrained
models are available via \url{https://www.github.com/vcheplygina/cats-scans}",['Computer Vision and Pattern Recognition']
Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue,"A goal-oriented visual dialogue involves multi-turn interactions between two
agents, Questioner and Oracle. During which, the answer given by Oracle is of
great significance, as it provides golden response to what Questioner concerns.
Based on the answer, Questioner updates its belief on target visual content and
further raises another question. Notably, different answers drive into
different visual beliefs and future questions. However, existing methods always
indiscriminately encode answers after much longer questions, resulting in a
weak utilization of answers. In this paper, we propose an Answer-Driven Visual
State Estimator (ADVSE) to impose the effects of different answers on visual
states. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture
the answer-driven effect on visual attention by sharpening question-related
attention and adjusting it by answer-based logical operation at each turn. Then
based on the focusing attention, we get the visual state estimation by
Conditional Visual Information Fusion (CVIF), where overall information and
difference information are fused conditioning on the question-answer state. We
evaluate the proposed ADVSE to both question generator and guesser tasks on the
large-scale GuessWhat?! dataset and achieve the state-of-the-art performances
on both tasks. The qualitative results indicate that the ADVSE boosts the agent
to generate highly efficient questions and obtains reliable visual attentions
during the reasonable question generation and guess processes.",['Computer Vision and Pattern Recognition']
Night vision obstacle detection and avoidance based on Bio-Inspired Vision Sensors,"Moving towards autonomy, unmanned vehicles rely heavily on state-of-the-art
collision avoidance systems (CAS). However, the detection of obstacles
especially during night-time is still a challenging task since the lighting
conditions are not sufficient for traditional cameras to function properly.
Therefore, we exploit the powerful attributes of event-based cameras to perform
obstacle detection in low lighting conditions. Event cameras trigger events
asynchronously at high output temporal rate with high dynamic range of up to
120 $dB$. The algorithm filters background activity noise and extracts objects
using robust Hough transform technique. The depth of each detected object is
computed by triangulating 2D features extracted utilising LC-Harris. Finally,
asynchronous adaptive collision avoidance (AACA) algorithm is applied for
effective avoidance. Qualitative evaluation is compared using event-camera and
traditional camera.","['Computer Vision and Pattern Recognition', 'Robotics']"
Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations,"The objective of this paper is to perform audio-visual sound source
separation, i.e.~to separate component audios from a mixture based on the
videos of sound sources. Moreover, we aim to pinpoint the source location in
the input video sequence. Recent works have shown impressive audio-visual
separation results when using prior knowledge of the source type (e.g. human
playing instrument) and pre-trained motion detectors (e.g. keypoints or optical
flows). However, at the same time, the models are limited to a certain
application domain. In this paper, we address these limitations and make the
following contributions: i) we propose a two-stage architecture, called
Appearance and Motion network (AMnet), where the stages specialise to
appearance and motion cues, respectively. The entire system is trained in a
self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME)
framework to explicitly represent the motions that related to sound; iii) we
propose an audio-motion transformer architecture for audio and motion feature
fusion; iv) we demonstrate state-of-the-art performance on two challenging
datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained
keypoint detectors or optical flow estimators. Project page:
https://ly-zhu.github.io/self-supervised-motion-representations",['Computer Vision and Pattern Recognition']
SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic Association and Salient Point Clustering Optimization,"We propose a novel 3D point cloud segmentation framework named SASO, which
jointly performs semantic and instance segmentation tasks. For semantic
segmentation task, inspired by the inherent correlation among objects in
spatial context, we propose a Multi-scale Semantic Association (MSA) module to
explore the constructive effects of the semantic context information. For
instance segmentation task, different from previous works that utilize
clustering only in inference procedure, we propose a Salient Point Clustering
Optimization (SPCO) module to introduce a clustering procedure into the
training process and impel the network focusing on points that are difficult to
be distinguished. In addition, because of the inherent structures of indoor
scenes, the imbalance problem of the category distribution is rarely considered
but severely limits the performance of 3D scene perception. To address this
issue, we introduce an adaptive Water Filling Sampling (WFS) algorithm to
balance the category distribution of training data. Extensive experiments
demonstrate that our method outperforms the state-of-the-art methods on
benchmark datasets in both semantic segmentation and instance segmentation
tasks.","['Computer Vision and Pattern Recognition', 'Robotics']"
Multi-Target Multi-Camera Tracking of Vehicles using Metadata-Aided Re-ID and Trajectory-Based Camera Link Model,"In this paper, we propose a novel framework for multi-target multi-camera
tracking (MTMCT) of vehicles based on metadata-aided re-identification
(MA-ReID) and the trajectory-based camera link model (TCLM). Given a video
sequence and the corresponding frame-by-frame vehicle detections, we first
address the isolated tracklets issue from single camera tracking (SCT) by the
proposed traffic-aware single-camera tracking (TSCT). Then, after automatically
constructing the TCLM, we solve MTMCT by the MA-ReID. The TCLM is generated
from camera topological configuration to obtain the spatial and temporal
information to improve the performance of MTMCT by reducing the candidate
search of ReID. We also use the temporal attention model to create more
discriminative embeddings of trajectories from each camera to achieve robust
distance measures for vehicle ReID. Moreover, we train a metadata classifier
for MTMCT to obtain the metadata feature, which is concatenated with the
temporal attention based embeddings. Finally, the TCLM and hierarchical
clustering are jointly applied for global ID assignment. The proposed method is
evaluated on the CityFlow dataset, achieving IDF1 76.77%, which outperforms the
state-of-the-art MTMCT methods.",['Computer Vision and Pattern Recognition']
Elastic registration based on compliance analysis and biomechanical graph matching,"An automatic elastic registration method suited for vascularized organs is
proposed. The vasculature in both the preoperative and intra-operative images
is represented as a graph. A typical application of this method is the fusion
of pre-operative information onto the organ during surgery, to compensate for
the limited details provided by the intra-operative imaging modality (e.g.
CBCT) and to cope with changes in the shape of the organ. Due to image
modalities differences and organ deformation, each graph has a different
topology and shape. The Adaptive Compliance Graph Matching (ACGM) method
presented does not require any manual initialization, handles intra-operative
nonrigid deformations of up to 65 mm and computes a complete displacement field
over the organ from only the matched vasculature. ACGM is better than the
previous Biomechanical Graph Matching method 3 (BGM) because it uses an
efficient biomechanical vascularized liver model to compute the organ's
transformation and the vessels bifurcations compliance. This allows to
efficiently find the best graph matches with a novel compliance-based adaptive
search. These contributions are evaluated on ten realistic synthetic and two
real porcine automatically segmented datasets. ACGM obtains better target
registration error (TRE) than BGM, with an average TRE in the real datasets of
4.2 mm compared to 6.5 mm, respectively. It also is up to one order of
magnitude faster, less dependent on the parameters used and more robust to
noise.",['Computer Vision and Pattern Recognition']
Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion,"Learning joint embedding space for various modalities is of vital importance
for multimodal fusion. Mainstream modality fusion approaches fail to achieve
this goal, leaving a modality gap which heavily affects cross-modal fusion. In
this paper, we propose a novel adversarial encoder-decoder-classifier framework
to learn a modality-invariant embedding space. Since the distributions of
various modalities vary in nature, to reduce the modality gap, we translate the
distributions of source modalities into that of target modality via their
respective encoders using adversarial training. Furthermore, we exert
additional constraints on embedding space by introducing reconstruction loss
and classification loss. Then we fuse the encoded representations using
hierarchical graph neural network which explicitly explores unimodal, bimodal
and trimodal interactions in multi-stage. Our method achieves state-of-the-art
performance on multiple datasets. Visualization of the learned embeddings
suggests that the joint embedding space learned by our method is
discriminative. code is available at:
\url{https://github.com/TmacMai/ARGF_multimodal_fusion}","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Multimedia']"
Superpixel Segmentation using Dynamic and Iterative Spanning Forest,"As constituent parts of image objects, superpixels can improve several
higher-level operations. However, image segmentation methods might have their
accuracy seriously compromised for reduced numbers of superpixels. We have
investigated a solution based on the Iterative Spanning Forest (ISF) framework.
In this work, we present Dynamic ISF (DISF) -- a method based on the following
steps. (a) It starts from an image graph and a seed set with considerably more
pixels than the desired number of superpixels. (b) The seeds compete among
themselves, and each seed conquers its most closely connected pixels, resulting
in an image partition (spanning forest) with connected superpixels. In step
(c), DISF assigns relevance values to seeds based on superpixel analysis and
removes the most irrelevant ones. Steps (b) and (c) are repeated until the
desired number of superpixels is reached. DISF has the chance to reconstruct
relevant edges after each iteration, when compared to region merging
algorithms. As compared to other seed-based superpixel methods, DISF is more
likely to find relevant seeds. It also introduces dynamic arc-weight estimation
in the ISF framework for more effective superpixel delineation, and we
demonstrate all results on three datasets with distinct object properties.",['Computer Vision and Pattern Recognition']
Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination,"Although unsupervised feature learning has demonstrated its advantages to
reducing the workload of data labeling and network design in many fields,
existing unsupervised 3D learning methods still cannot offer a generic network
for various shape analysis tasks with competitive performance to supervised
methods. In this paper, we propose an unsupervised method for learning a
generic and efficient shape encoding network for different shape analysis
tasks. The key idea of our method is to jointly encode and learn shape and
point features from unlabeled 3D point clouds. For this purpose, we adapt
HR-Net to octree-based convolutional neural networks for jointly encoding shape
and point features with fused multiresolution subnetworks and design a
simple-yet-efficient Multiresolution Instance Discrimination (MID) loss for
jointly learning the shape and point features. Our network takes a 3D point
cloud as input and output both shape and point features. After training, the
network is concatenated with simple task-specific back-end layers and
fine-tuned for different shape analysis tasks. We evaluate the efficacy and
generality of our method and validate our network and loss design with a set of
shape analysis tasks, including shape classification, semantic shape
segmentation, as well as shape registration tasks. With simple back-ends, our
network demonstrates the best performance among all unsupervised methods and
achieves competitive performance to supervised methods, especially in tasks
with a small labeled dataset. For fine-grained shape segmentation, our method
even surpasses existing supervised methods by a large margin.","['Computer Vision and Pattern Recognition', 'Graphics']"
TempNodeEmb:Temporal Node Embedding considering temporal edge influence matrix,"Understanding the evolutionary patterns of real-world evolving complex
systems such as human interactions, transport networks, biological
interactions, and computer networks has important implications in our daily
lives. Predicting future links among the nodes in such networks reveals an
important aspect of the evolution of temporal networks. To analyse networks,
they are mapped to adjacency matrices, however, a single adjacency matrix
cannot represent complex relationships (e.g. temporal pattern), and therefore,
some approaches consider a simplified representation of temporal networks but
in high-dimensional and generally sparse matrices. As a result, adjacency
matrices cannot be directly used by machine learning models for making network
or node level predictions. To overcome this problem, automated frameworks are
proposed for learning low-dimensional vectors for nodes or edges, as
state-of-the-art techniques in predicting temporal patterns in networks such as
link prediction. However, these models fail to consider temporal dimensions of
the networks. This gap motivated us to propose in this research a new node
embedding technique which exploits the evolving nature of the networks
considering a simple three-layer graph neural network at each time step, and
extracting node orientation by Given's angle method. To prove our proposed
algorithm's efficiency, we evaluated the efficiency of our proposed algorithm
against six state-of-the-art benchmark network embedding models, on four real
temporal networks data, and the results show our model outperforms other
methods in predicting future links in temporal networks.","['Machine Learning', 'Social and Information Networks']"
Fast and Accurate: Structure Coherence Component for Face Alignment,"In this paper, we propose a fast and accurate coordinate regression method
for face alignment. Unlike most existing facial landmark regression methods
which usually employ fully connected layers to convert feature maps into
landmark coordinate, we present a structure coherence component to explicitly
take the relation among facial landmarks into account. Due to the geometric
structure of human face, structure coherence between different facial parts
provides important cues for effectively localizing facial landmarks. However,
the dense connection in the fully connected layers overuses such coherence,
making the important cues unable to be distinguished from all connections.
Instead, our structure coherence component leverages a dynamic sparse graph
structure to passing features among the most related landmarks. Furthermore, we
propose a novel objective function, named Soft Wing loss, to improve the
accuracy. Extensive experiments on three popular benchmarks, including WFLW,
COFW and 300W, demonstrate the effectiveness of the proposed method, achieving
state-of-the-art performance with fast speed. Our approach is especially robust
to challenging cases resulting in impressively low failure rate (0% and 2.88%)
in COFW and WFLW datasets.",['Computer Vision and Pattern Recognition']
Coding for Distributed Multi-Agent Reinforcement Learning,"This paper aims to mitigate straggler effects in synchronous distributed
learning for multi-agent reinforcement learning (MARL) problems. Stragglers
arise frequently in a distributed learning system, due to the existence of
various system disturbances such as slow-downs or failures of compute nodes and
communication bottlenecks. To resolve this issue, we propose a coded
distributed learning framework, which speeds up the training of MARL algorithms
in the presence of stragglers, while maintaining the same accuracy as the
centralized approach. As an illustration, a coded distributed version of the
multi-agent deep deterministic policy gradient(MADDPG) algorithm is developed
and evaluated. Different coding schemes, including maximum distance separable
(MDS)code, random sparse code, replication-based code, and regular low density
parity check (LDPC) code are also investigated. Simulations in several
multi-robot problems demonstrate the promising performance of the proposed
framework.",['Machine Learning']
Rethinking Interactive Image Segmentation: Feature Space Annotation,"Despite the progress of interactive image segmentation methods, high-quality
pixel-level annotation is still time-consuming and laborious -- a bottleneck
for several deep learning applications. We take a step back to propose
interactive and simultaneous segment annotation from multiple images guided by
feature space projection and optimized by metric learning as the labeling
progresses. This strategy is in stark contrast to existing interactive
segmentation methodologies, which perform annotation in the image domain. We
show that our approach can surpass the accuracy of state-of-the-art methods in
foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, it
achieves 91.5\% accuracy in a known semantic segmentation dataset, Cityscapes,
being 74.75 times faster than the original annotation procedure. The appendix
presents additional qualitative results. Code and video demonstration will be
released upon publication.",['Computer Vision and Pattern Recognition']
LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,"We present LR-GAN: an adversarial image generation model which takes scene
structure and context into account. Unlike previous generative adversarial
networks (GANs), the proposed GAN learns to generate image background and
foregrounds separately and recursively, and stitch the foregrounds on the
background in a contextually relevant manner to produce a complete natural
image. For each foreground, the model learns to generate its appearance, shape
and pose. The whole model is unsupervised, and is trained in an end-to-end
manner with gradient descent methods. The experiments demonstrate that LR-GAN
can generate more natural images with objects that are more human recognizable
than DCGAN.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Learned versus Hand-Designed Feature Representations for 3d Agglomeration,"For image recognition and labeling tasks, recent results suggest that machine
learning methods that rely on manually specified feature representations may be
outperformed by methods that automatically derive feature representations based
on the data. Yet for problems that involve analysis of 3d objects, such as mesh
segmentation, shape retrieval, or neuron fragment agglomeration, there remains
a strong reliance on hand-designed feature descriptors. In this paper, we
evaluate a large set of hand-designed 3d feature descriptors alongside features
learned from the raw data using both end-to-end and unsupervised learning
techniques, in the context of agglomeration of 3d neuron fragments. By
combining unsupervised learning techniques with a novel dynamic pooling scheme,
we show how pure learning-based methods are for the first time competitive with
hand-designed 3d shape descriptors. We investigate data augmentation strategies
for dramatically increasing the size of the training set, and show how
combining both learned and hand-designed features leads to the highest
accuracy.",['Computer Vision and Pattern Recognition']
Learning to Imagine Manipulation Goals for Robot Task Planning,"Prospection is an important part of how humans come up with new task plans,
but has not been explored in depth in robotics. Predicting multiple task-level
is a challenging problem that involves capturing both task semantics and
continuous variability over the state of the world. Ideally, we would combine
the ability of machine learning to leverage big data for learning the semantics
of a task, while using techniques from task planning to reliably generalize to
new environment. In this work, we propose a method for learning a model
encoding just such a representation for task planning. We learn a neural net
that encodes the $k$ most likely outcomes from high level actions from a given
world. Our approach creates comprehensible task plans that allow us to predict
changes to the environment many time steps into the future. We demonstrate this
approach via application to a stacking task in a cluttered environment, where
the robot must select between different colored blocks while avoiding
obstacles, in order to perform a task. We also show results on a simple
navigation task. Our algorithm generates realistic image and pose predictions
at multiple points in a given task.","['Machine Learning', 'Robotics']"
Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation,"Recently, referring image segmentation has aroused widespread interest.
Previous methods perform the multi-modal fusion between language and vision at
the decoding side of the network. And, linguistic feature interacts with visual
feature of each scale separately, which ignores the continuous guidance of
language to multi-scale visual features. In this work, we propose an encoder
fusion network (EFN), which transforms the visual encoder into a multi-modal
feature learning network, and uses language to refine the multi-modal features
progressively. Moreover, a co-attention mechanism is embedded in the EFN to
realize the parallel update of multi-modal features, which can promote the
consistent of the cross-modal information representation in the semantic space.
Finally, we propose a boundary enhancement module (BEM) to make the network pay
more attention to the fine structure. The experiment results on four benchmark
datasets demonstrate that the proposed approach achieves the state-of-the-art
performance under different evaluation metrics without any post-processing.",['Computer Vision and Pattern Recognition']
MetaMix: Improved Meta-Learning with Interpolation-based Consistency Regularization,"Model-Agnostic Meta-Learning (MAML) and its variants are popular few-shot
classification methods. They train an initializer across a variety of sampled
learning tasks (also known as episodes) such that the initialized model can
adapt quickly to new tasks. However, current MAML-based algorithms have
limitations in forming generalizable decision boundaries. In this paper, we
propose an approach called MetaMix. It generates virtual feature-target pairs
within each episode to regularize the backbone models. MetaMix can be
integrated with any of the MAML-based algorithms and learn the decision
boundaries generalizing better to new tasks. Experiments on the mini-ImageNet,
CUB, and FC100 datasets show that MetaMix improves the performance of
MAML-based algorithms and achieves state-of-the-art result when integrated with
Meta-Transfer Learning.",['Computer Vision and Pattern Recognition']
GMAIR: Unsupervised Object Detection Based on Spatial Attention and Gaussian Mixture,"Recent studies on unsupervised object detection based on spatial attention
have achieved promising results. Models, such as AIR and SPAIR, output ""what""
and ""where"" latent variables that represent the attributes and locations of
objects in a scene, respectively. Most of the previous studies concentrate on
the ""where"" localization performance; however, we claim that acquiring ""what""
object attributes is also essential for representation learning. This paper
presents a framework, GMAIR, for unsupervised object detection. It incorporates
spatial attention and a Gaussian mixture in a unified deep generative model.
GMAIR can locate objects in a scene and simultaneously cluster them without
supervision. Furthermore, we analyze the ""what"" latent variables and clustering
process. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and
show that GMAIR achieves competitive results on localization and clustering
compared to state-of-the-art methods.",['Computer Vision and Pattern Recognition']
Policy Manifold Search for Improving Diversity-based Neuroevolution,"Diversity-based approaches have recently gained popularity as an alternative
paradigm to performance-based policy search. A popular approach from this
family, Quality-Diversity (QD), maintains a collection of high-performing
policies separated in the diversity-metric space, defined based on policies'
rollout behaviours. When policies are parameterised as neural networks, i.e.
Neuroevolution, QD tends to not scale well with parameter space dimensionality.
Our hypothesis is that there exists a low-dimensional manifold embedded in the
policy parameter space, containing a high density of diverse and feasible
policies. We propose a novel approach to diversity-based policy search via
Neuroevolution, that leverages learned latent representations of the policy
parameters which capture the local structure of the data. Our approach
iteratively collects policies according to the QD framework, in order to (i)
build a collection of diverse policies, (ii) use it to learn a latent
representation of the policy parameters, (iii) perform policy search in the
learned latent space. We use the Jacobian of the inverse transformation
(i.e.reconstruction function) to guide the search in the latent space. This
ensures that the generated samples remain in the high-density regions of the
original space, after reconstruction. We evaluate our contributions on three
continuous control tasks in simulated environments, and compare to
diversity-based baselines. The findings suggest that our approach yields a more
efficient and robust policy search process.","['Machine Learning', 'Neural and Evolutionary Computing']"
"Jacks of All Trades, Masters Of None: Addressing Distributional Shift and Obtrusiveness via Transparent Patch Attacks","We focus on the development of effective adversarial patch attacks and -- for
the first time -- jointly address the antagonistic objectives of attack success
and obtrusiveness via the design of novel semi-transparent patches. This work
is motivated by our pursuit of a systematic performance analysis of patch
attack robustness with regard to geometric transformations. Specifically, we
first elucidate a) key factors underpinning patch attack success and b) the
impact of distributional shift between training and testing/deployment when
cast under the Expectation over Transformation (EoT) formalism. By focusing our
analysis on three principal classes of transformations (rotation, scale, and
location), our findings provide quantifiable insights into the design of
effective patch attacks and demonstrate that scale, among all factors,
significantly impacts patch attack success. Working from these findings, we
then focus on addressing how to overcome the principal limitations of scale for
the deployment of attacks in real physical settings: namely the obtrusiveness
of large patches. Our strategy is to turn to the novel design of
irregularly-shaped, semi-transparent partial patches which we construct via a
new optimization process that jointly addresses the antagonistic goals of
mitigating obtrusiveness and maximizing effectiveness. Our study -- we hope --
will help encourage more focus in the community on the issues of obtrusiveness,
scale, and success in patch attacks.","['Computer Vision and Pattern Recognition', 'Cryptography and Security', 'Machine Learning']"
Dynamic Image Restoration and Fusion Based on Dynamic Degradation,"The deep-learning-based image restoration and fusion methods have achieved
remarkable results. However, the existing restoration and fusion methods paid
little research attention to the robustness problem caused by dynamic
degradation. In this paper, we propose a novel dynamic image restoration and
fusion neural network, termed as DDRF-Net, which is capable of solving two
problems, i.e., static restoration and fusion, dynamic degradation. In order to
solve the static fusion problem of existing methods, dynamic convolution is
introduced to learn dynamic restoration and fusion weights. In addition, a
dynamic degradation kernel is proposed to improve the robustness of image
restoration and fusion. Our network framework can effectively combine image
degradation with image fusion tasks, provide more detailed information for
image fusion tasks through image restoration loss, and optimize image
restoration tasks through image fusion loss. Therefore, the stumbling blocks of
deep learning in image fusion, e.g., static fusion weight and specifically
designed network architecture, are greatly mitigated. Extensive experiments
show that our method is more superior compared with the state-of-the-art
methods.",['Computer Vision and Pattern Recognition']
Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning Approach,"Face attribute estimation has many potential applications in video
surveillance, face retrieval, and social media. While a number of methods have
been proposed for face attribute estimation, most of them did not explicitly
consider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal
and holistic vs. local) during feature representation learning. In this paper,
we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate
multiple heterogeneous attributes from a single face image. In DMTL, we tackle
attribute correlation and heterogeneity with convolutional neural networks
(CNNs) consisting of shared feature learning for all the attributes, and
category-specific feature learning for heterogeneous attributes. We also
introduce an unconstrained face database (LFW+), an extension of public-domain
LFW, with heterogeneous demographic attributes (age, gender, and race) obtained
via crowdsourcing. Experimental results on benchmarks with multiple face
attributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed
approach has superior performance compared to state of the art. Finally,
evaluations on a public-domain face database (LAP) with a single attribute show
that the proposed approach has excellent generalization ability.",['Computer Vision and Pattern Recognition']
Hierarchical pixel clustering for image segmentation,"In the paper a piecewise constant image approximations of sequential number
of pixel clusters or segments are treated. A majorizing of optimal
approximation sequence by hierarchical sequence of image approximations is
studied. Transition from pixel clustering to image segmentation by reducing of
segment numbers in clusters is provided. Algorithms are proved by elementary
formulas.",['Computer Vision and Pattern Recognition']
3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks,"We propose a method for reconstructing 3D shapes from 2D sketches in the form
of line drawings. Our method takes as input a single sketch, or multiple
sketches, and outputs a dense point cloud representing a 3D reconstruction of
the input sketch(es). The point cloud is then converted into a polygon mesh. At
the heart of our method lies a deep, encoder-decoder network. The encoder
converts the sketch into a compact representation encoding shape information.
The decoder converts this representation into depth and normal maps capturing
the underlying surface from several output viewpoints. The multi-view maps are
then consolidated into a 3D point cloud by solving an optimization problem that
fuses depth and normals across all viewpoints. Based on our experiments,
compared to other methods, such as volumetric networks, our architecture offers
several advantages, including more faithful reconstruction, higher output
surface resolution, better preservation of topology and shape structure.","['Computer Vision and Pattern Recognition', 'Graphics']"
Deformable PV-RCNN: Improving 3D Object Detection with Learned Deformations,"We present Deformable PV-RCNN, a high-performing point-cloud based 3D object
detector. Currently, the proposal refinement methods used by the
state-of-the-art two-stage detectors cannot adequately accommodate differing
object scales, varying point-cloud density, part-deformation and clutter. We
present a proposal refinement module inspired by 2D deformable convolution
networks that can adaptively gather instance-specific features from locations
where informative content exists. We also propose a simple context gating
mechanism which allows the keypoints to select relevant context information for
the refinement stage. We show state-of-the-art results on the KITTI dataset.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Improving Visual Reasoning by Exploiting The Knowledge in Texts,"This paper presents a new framework for training image-based classifiers from
a combination of texts and images with very few labels. We consider a
classification framework with three modules: a backbone, a relational reasoning
component, and a classification component. While the backbone can be trained
from unlabeled images by self-supervised learning, we can fine-tune the
relational reasoning and the classification components from external sources of
knowledge instead of annotated images. By proposing a transformer-based model
that creates structured knowledge from textual input, we enable the utilization
of the knowledge in texts. We show that, compared to the supervised baselines
with 1% of the annotated images, we can achieve ~8x more accurate results in
scene graph classification, ~3x in object classification, and ~1.5x in
predicate classification.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
FFAVOD: Feature Fusion Architecture for Video Object Detection,"A significant amount of redundancy exists between consecutive frames of a
video. Object detectors typically produce detections for one image at a time,
without any capabilities for taking advantage of this redundancy. Meanwhile,
many applications for object detection work with videos, including intelligent
transportation systems, advanced driver assistance systems and video
surveillance. Our work aims at taking advantage of the similarity between video
frames to produce better detections. We propose FFAVOD, standing for feature
fusion architecture for video object detection. We first introduce a novel
video object detection architecture that allows a network to share feature maps
between nearby frames. Second, we propose a feature fusion module that learns
to merge feature maps to enhance them. We show that using the proposed
architecture and the fusion module can improve the performance of three base
object detectors on two object detection benchmarks containing sequences of
moving road users. Additionally, to further increase performance, we propose an
improvement to the SpotNet attention module. Using our architecture on the
improved SpotNet detector, we obtain the state-of-the-art performance on the
UA-DETRAC public benchmark as well as on the UAVDT dataset. Code is available
at https://github.com/hu64/FFAVOD.",['Computer Vision and Pattern Recognition']
Adversarial Attacks Beyond the Image Space,"Generating adversarial examples is an intriguing problem and an important way
of understanding the working mechanism of deep neural networks. Most existing
approaches generated perturbations in the image space, i.e., each pixel can be
modified independently. However, in this paper we pay special attention to the
subset of adversarial examples that correspond to meaningful changes in 3D
physical properties (like rotation and translation, illumination condition,
etc.). These adversaries arguably pose a more serious concern, as they
demonstrate the possibility of causing neural network failure by easy
perturbations of real-world 3D objects and scenes.
  In the contexts of object classification and visual question answering, we
augment state-of-the-art deep neural networks that receive 2D input images with
a rendering module (either differentiable or not) in front, so that a 3D scene
(in the physical space) is rendered into a 2D image (in the image space), and
then mapped to a prediction (in the output space). The adversarial
perturbations can now go beyond the image space, and have clear meanings in the
3D physical world. Though image-space adversaries can be interpreted as
per-pixel albedo change, we verify that they cannot be well explained along
these physically meaningful dimensions, which often have a non-local effect.
But it is still possible to successfully attack beyond the image space on the
physical space, though this is more difficult than image-space attacks,
reflected in lower success rates and heavier perturbations required.",['Computer Vision and Pattern Recognition']
Cooperative Policy Learning with Pre-trained Heterogeneous Observation Representations,"Multi-agent reinforcement learning (MARL) has been increasingly explored to
learn the cooperative policy towards maximizing a certain global reward. Many
existing studies take advantage of graph neural networks (GNN) in MARL to
propagate critical collaborative information over the interaction graph, built
upon inter-connected agents. Nevertheless, the vanilla GNN approach yields
substantial defects in dealing with complex real-world scenarios since the
generic message passing mechanism is ineffective between heterogeneous vertices
and, moreover, simple message aggregation functions are incapable of accurately
modeling the combinational interactions from multiple neighbors. While adopting
complex GNN models with more informative message passing and aggregation
mechanisms can obviously benefit heterogeneous vertex representations and
cooperative policy learning, it could, on the other hand, increase the training
difficulty of MARL and demand more intense and direct reward signals compared
to the original global reward. To address these challenges, we propose a new
cooperative learning framework with pre-trained heterogeneous observation
representations. Particularly, we employ an encoder-decoder based graph
attention to learn the intricate interactions and heterogeneous representations
that can be more easily leveraged by MARL. Moreover, we design a pre-training
with local actor-critic algorithm to ease the difficulty in cooperative policy
learning. Extensive experiments over real-world scenarios demonstrate that our
new approach can significantly outperform existing MARL baselines as well as
operational research solutions that are widely-used in industry.","['Machine Learning', 'Artificial Intelligence']"
Learning to Paint With Model-based Deep Reinforcement Learning,"We show how to teach machines to paint like human painters, who can use a
small number of strokes to create fantastic paintings. By employing a neural
renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to
determine the position and color of each stroke and make long-term plans to
decompose texture-rich images into strokes. Experiments demonstrate that
excellent visual effects can be achieved using hundreds of strokes. The
training process does not require the experience of human painters or stroke
tracking data. The code is available at
https://github.com/hzwer/ICCV2019-LearningToPaint.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
Convolutional CRFs for Semantic Segmentation,"For the challenging semantic image segmentation task the most efficient
models have traditionally combined the structured modelling capabilities of
Conditional Random Fields (CRFs) with the feature extraction power of CNNs. In
more recent works however, CRF post-processing has fallen out of favour. We
argue that this is mainly due to the slow training and inference speeds of
CRFs, as well as the difficulty of learning the internal CRF parameters. To
overcome both issues we propose to add the assumption of conditional
independence to the framework of fully-connected CRFs. This allows us to
reformulate the inference in terms of convolutions, which can be implemented
highly efficiently on GPUs. Doing so speeds up inference and training by a
factor of more then 100. All parameters of the convolutional CRFs can easily be
optimized using backpropagation. To facilitating further CRF research we make
our implementation publicly available. Please visit:
https://github.com/MarvinTeichmann/ConvCRF",['Computer Vision and Pattern Recognition']
DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization,"In this paper, we propose a deep part-based model (DeePM) for symbiotic
object detection and semantic part localization. For this purpose, we annotate
semantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,
which provides information on object pose, occlusion, viewpoint and
functionality. DeePM is a latent graphical model based on the state-of-the-art
R-CNN framework, which learns an explicit representation of the object-part
configuration with flexible type sharing (e.g., a sideview horse head can be
shared by a fully-visible sideview horse and a highly truncated sideview horse
with head and neck only). For comparison, we also present an end-to-end
Object-Part (OP) R-CNN which learns an implicit feature representation for
jointly mapping an image ROI to the object and part bounding boxes. We evaluate
the proposed methods for both the object and part detection performance on
PASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in
detecting objects and parts. In addition, it obtains superior performance to
Fast and Faster R-CNNs in object detection.",['Computer Vision and Pattern Recognition']
Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?,"The design of neural network architectures is frequently either based on
human expertise using trial/error and empirical feedback or tackled via large
scale reinforcement learning strategies performed over distinct discrete
architecture choices. In the latter case, the optimization is often
non-differentiable and also not very amenable to derivative-free optimization
methods. Most methods in use today require sizable computational resources. And
if we want networks that additionally satisfy resource constraints, the above
challenges are exacerbated because the search must now balance accuracy with
certain budget constraints on resources. We formulate this problem as the
optimization of a set function -- we find that the empirical behavior of this
set function often (but not always) satisfies marginal gain and monotonicity
principles -- properties central to the idea of submodularity. Based on this
observation, we adapt algorithms within discrete optimization to obtain
heuristic schemes for neural network architecture search, where we have
resource constraints on the architecture. This simple scheme when applied on
CIFAR-100 and ImageNet, identifies resource-constrained architectures with
quantifiably better performance than current state-of-the-art models designed
for mobile devices. Specifically, we find high-performing architectures with
fewer parameters and computations by a search method that is much faster.",['Computer Vision and Pattern Recognition']
Improvements to context based self-supervised learning,"We develop a set of methods to improve on the results of self-supervised
learning using context. We start with a baseline of patch based arrangement
context learning and go from there. Our methods address some overt problems
such as chromatic aberration as well as other potential problems such as
spatial skew and mid-level feature neglect. We prevent problems with testing
generalization on common self-supervised benchmark tests by using different
datasets during our development. The results of our methods combined yield top
scores on all standard self-supervised benchmarks, including classification and
detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and ""linear
tests"" on the ImageNet and CSAIL Places datasets. We obtain an improvement over
our baseline method of between 4.0 to 7.1 percentage points on transfer
learning classification tests. We also show results on different standard
network architectures to demonstrate generalization as well as portability. All
data, models and programs are available at:
https://gdo-datasci.llnl.gov/selfsupervised/.","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Neural and Evolutionary Computing']"
Sketch-QNet: A Quadruplet ConvNet for Color Sketch-based Image Retrieval,"Architectures based on siamese networks with triplet loss have shown
outstanding performance on the image-based similarity search problem. This
approach attempts to discriminate between positive (relevant) and negative
(irrelevant) items. However, it undergoes a critical weakness. Given a query,
it cannot discriminate weakly relevant items, for instance, items of the same
type but different color or texture as the given query, which could be a
serious limitation for many real-world search applications. Therefore, in this
work, we present a quadruplet-based architecture that overcomes the
aforementioned weakness. Moreover, we present an instance of this quadruplet
network, which we call Sketch-QNet, to deal with the color sketch-based image
retrieval (CSBIR) problem, achieving new state-of-the-art results.",['Computer Vision and Pattern Recognition']
PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,"Recent research has shown that incorporating equivariance into neural network
architectures is very helpful, and there have been some works investigating the
equivariance of networks under group actions. However, as digital images and
feature maps are on the discrete meshgrid, corresponding
equivariance-preserving transformation groups are very limited. In this work,
we deal with this issue from the connection between convolutions and partial
differential operators (PDOs). In theory, assuming inputs to be smooth, we
transform PDOs and propose a system which is equivariant to a much more general
continuous group, the $n$-dimension Euclidean group. In implementation, we
discretize the system using the numerical schemes of PDOs, deriving
approximately equivariant convolutions (PDO-eConvs). Theoretically, the
approximation error of PDO-eConvs is of the quadratic order. It is the first
time that the error analysis is provided when the equivariance is approximate.
Extensive experiments on rotated MNIST and natural image classification show
that PDO-eConvs perform competitively yet use parameters much more efficiently.
Particularly, compared with Wide ResNets, our methods result in better results
using only 12.6% parameters.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Graph Based Over-Segmentation Methods for 3D Point Clouds,"Over-segmentation, or super-pixel generation, is a common preliminary stage
for many computer vision applications. New acquisition technologies enable the
capturing of 3D point clouds that contain color and geometrical information.
This 3D information introduces a new conceptual change that can be utilized to
improve the results of over-segmentation, which uses mainly color information,
and to generate clusters of points we call super-points. We consider a variety
of possible 3D extensions of the Local Variation (LV) graph based
over-segmentation algorithms, and compare them thoroughly. We consider
different alternatives for constructing the connectivity graph, for assigning
the edge weights, and for defining the merge criterion, which must now account
for the geometric information and not only color. Following this evaluation, we
derive a new generic algorithm for over-segmentation of 3D point clouds. We
call this new algorithm Point Cloud Local Variation (PCLV). The advantages of
the new over-segmentation algorithm are demonstrated on both outdoor and
cluttered indoor scenes. Performance analysis of the proposed approach compared
to state-of-the-art 2D and 3D over-segmentation algorithms shows significant
improvement according to the common performance measures.",['Computer Vision and Pattern Recognition']
Stabilizing Generative Adversarial Networks: A Survey,"Generative Adversarial Networks (GANs) are a type of generative model which
have received much attention due to their ability to model complex real-world
data. Despite their recent successes, the process of training GANs remains
challenging, suffering from instability problems such as non-convergence,
vanishing or exploding gradients, and mode collapse. In recent years, a diverse
set of approaches have been proposed which focus on stabilizing the GAN
training procedure. The purpose of this survey is to provide a comprehensive
overview of the GAN training stabilization methods which can be found in the
literature. We discuss the advantages and disadvantages of each approach, offer
a comparative summary, and conclude with a discussion of open problems.",['Machine Learning']
Deep Spatial Transformation for Pose-Guided Person Image Generation and Animation,"Pose-guided person image generation and animation aim to transform a source
person image to target poses. These tasks require spatial manipulation of
source data. However, Convolutional Neural Networks are limited by the lack of
ability to spatially transform the inputs. In this paper, we propose a
differentiable global-flow local-attention framework to reassemble the inputs
at the feature level. This framework first estimates global flow fields between
sources and targets. Then, corresponding local source feature patches are
sampled with content-aware local attention coefficients. We show that our
framework can spatially transform the inputs in an efficient manner. Meanwhile,
we further model the temporal consistency for the person image animation task
to generate coherent videos. The experiment results of both image generation
and animation tasks demonstrate the superiority of our model. Besides,
additional results of novel view synthesis and face image animation show that
our model is applicable to other tasks requiring spatial transformation. The
source code of our project is available at
https://github.com/RenYurui/Global-Flow-Local-Attention.",['Computer Vision and Pattern Recognition']
Primitive Representation Learning for Scene Text Recognition,"Scene text recognition is a challenging task due to diverse variations of
text instances in natural scene images. Conventional methods based on
CNN-RNN-CTC or encoder-decoder with attention mechanism may not fully
investigate stable and efficient feature representations for multi-oriented
scene texts. In this paper, we propose a primitive representation learning
method that aims to exploit intrinsic representations of scene text images. We
model elements in feature maps as the nodes of an undirected graph. A pooling
aggregator and a weighted aggregator are proposed to learn primitive
representations, which are transformed into high-level visual text
representations by graph convolutional networks. A Primitive REpresentation
learning Network (PREN) is constructed to use the visual text representations
for parallel decoding. Furthermore, by integrating visual text representations
into an encoder-decoder model with the 2D attention mechanism, we propose a
framework called PREN2D to alleviate the misalignment problem in
attention-based methods. Experimental results on both English and Chinese scene
text recognition tasks demonstrate that PREN keeps a balance between accuracy
and efficiency, while PREN2D achieves state-of-the-art performance.",['Computer Vision and Pattern Recognition']
TCDesc: Learning Topology Consistent Descriptors for Image Matching,"The constraint of neighborhood consistency or local consistency is widely
used for robust image matching. In this paper, we focus on learning
neighborhood topology consistent descriptors (TCDesc), while former works of
learning descriptors, such as HardNet and DSM, only consider point-to-point
Euclidean distance among descriptors and totally neglect neighborhood
information of descriptors. To learn topology consistent descriptors, first we
propose the linear combination weights to depict the topological relationship
between center descriptor and its kNN descriptors, where the difference between
center descriptor and the linear combination of its kNN descriptors is
minimized. Then we propose the global mapping function which maps the local
linear combination weights to the global topology vector and define the
topology distance of matching descriptors as l1 distance between their topology
vectors. Last we employ adaptive weighting strategy to jointly minimize
topology distance and Euclidean distance, which automatically adjust the weight
or attention of two distances in triplet loss. Our method has the following two
advantages: (1) We are the first to consider neighborhood information of
descriptors, while former works mainly focus on neighborhood consistency of
feature points; (2) Our method can be applied in any former work of learning
descriptors by triplet loss. Experimental results verify the generalization of
our method: We can improve the performances of both HardNet and DSM on several
benchmarks.",['Computer Vision and Pattern Recognition']
3D_DEN: Open-ended 3D Object Recognition using Dynamically Expandable Networks,"Service robots, in general, have to work independently and adapt to the
dynamic changes happening in the environment in real-time. One important aspect
in such scenarios is to continually learn to recognize newer object categories
when they become available. This combines two main research problems namely
continual learning and 3D object recognition. Most of the existing research
approaches include the use of deep Convolutional Neural Networks (CNNs)
focusing on image datasets. A modified approach might be needed for continually
learning 3D object categories. A major concern in using CNNs is the problem of
catastrophic forgetting when a model tries to learn a new task. Despite various
proposed solutions to mitigate this problem, there still exist some downsides
of such solutions, e.g., computational complexity, especially when learning
substantial number of tasks. These downsides can pose major problems in robotic
scenarios where real-time response plays an essential role. Towards addressing
this challenge, we propose a new deep transfer learning approach based on a
dynamic architectural method to make robots capable of open-ended learning
about new 3D object categories. Furthermore, we make sure that the mentioned
downsides are minimized to a great extent. Experimental results showed that the
proposed model outperformed state-of-the-art approaches with regards to
accuracy and also substantially minimizes computational overhead.","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Robotics']"
Graph Contrastive Learning with Augmentations,"Generalizable, transferrable, and robust representation learning on
graph-structured data remains a challenge for current graph neural networks
(GNNs). Unlike what has been developed for convolutional neural networks (CNNs)
for image data, self-supervised learning and pre-training are less explored for
GNNs. In this paper, we propose a graph contrastive learning (GraphCL)
framework for learning unsupervised representations of graph data. We first
design four types of graph augmentations to incorporate various priors. We then
systematically study the impact of various combinations of graph augmentations
on multiple datasets, in four different settings: semi-supervised,
unsupervised, and transfer learning as well as adversarial attacks. The results
show that, even without tuning augmentation extents nor using sophisticated GNN
architectures, our GraphCL framework can produce graph representations of
similar or better generalizability, transferrability, and robustness compared
to state-of-the-art methods. We also investigate the impact of parameterized
graph augmentation extents and patterns, and observe further performance gains
in preliminary experiments. Our codes are available at
https://github.com/Shen-Lab/GraphCL.","['Machine Learning', 'Artificial Intelligence']"
"""Train one, Classify one, Teach one"" -- Cross-surgery transfer learning for surgical step recognition","Prior work demonstrated the ability of machine learning to automatically
recognize surgical workflow steps from videos. However, these studies focused
on only a single type of procedure. In this work, we analyze, for the first
time, surgical step recognition on four different laparoscopic surgeries:
Cholecystectomy, Right Hemicolectomy, Sleeve Gastrectomy, and Appendectomy.
Inspired by the traditional apprenticeship model, in which surgical training is
based on the Halstedian method, we paraphrase the ""see one, do one, teach one""
approach for the surgical intelligence domain as ""train one, classify one,
teach one"". In machine learning, this approach is often referred to as transfer
learning. To analyze the impact of transfer learning across different
laparoscopic procedures, we explore various time-series architectures and
examine their performance on each target domain. We introduce a new
architecture, the Time-Series Adaptation Network (TSAN), an architecture
optimized for transfer learning of surgical step recognition, and we show how
TSAN can be pre-trained using self-supervised learning on a Sequence Sorting
task. Such pre-training enables TSAN to learn workflow steps of a new
laparoscopic procedure type from only a small number of labeled samples from
the target procedure. Our proposed architecture leads to better performance
compared to other possible architectures, reaching over 90% accuracy when
transferring from laparoscopic Cholecystectomy to the other three procedure
types.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation,"Semantic image segmentation plays a pivotal role in many vision applications
including autonomous driving and medical image analysis. Most of the former
approaches move towards enhancing the performance in terms of accuracy with a
little awareness of computational efficiency. In this paper, we introduce
LiteSeg, a lightweight architecture for semantic image segmentation. In this
work, we explore a new deeper version of Atrous Spatial Pyramid Pooling module
(ASPP) and apply short and long residual connections, and depthwise separable
convolution, resulting in a faster and efficient model. LiteSeg architecture is
introduced and tested with multiple backbone networks as Darknet19, MobileNet,
and ShuffleNet to provide multiple trade-offs between accuracy and
computational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone
network, achieves an accuracy of 67.81% mean intersection over union at 161
frames per second with $640 \times 360$ resolution on the Cityscapes dataset.",['Computer Vision and Pattern Recognition']
Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware,"Real-world perception systems in many cases build on hardware with limited
resources to adhere to cost and power limitations of their carrying system.
Deploying deep neural networks on resource-constrained hardware became possible
with model compression techniques, as well as efficient and hardware-aware
architecture design. However, model adaptation is additionally required due to
the diverse operation environments. In this work, we address the problem of
training deep neural networks on resource-constrained hardware in the context
of visual domain adaptation. We select the task of monocular depth estimation
where our goal is to transform a pre-trained model to the target's domain data.
While the source domain includes labels, we assume an unlabelled target domain,
as it happens in real-world applications. Then, we present an adversarial
learning approach that is adapted for training on the device with limited
resources. Since visual domain adaptation, i.e. neural network training, has
not been previously explored for resource-constrained hardware, we present the
first feasibility study for image-based depth estimation. Our experiments show
that visual domain adaptation is relevant only for efficient network
architectures and training sets at the order of a few hundred samples. Models
and code are publicly available.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation,"Monocular 3D object detection is a promising research topic for the
intelligent perception systems of autonomous driving. In this work, a
single-stage keypoint-based network, named as FADNet, is presented to address
the task of monocular 3D object detection. In contrast to previous
keypoint-based methods which adopt identical layouts for output branches, we
propose to divide the output modalities into different groups according to the
estimating difficulty, whereby different groups are treated differently by
sequential feature association. Another contribution of this work is the
strategy of depth hint augmentation. To provide characterized depth patterns as
hints for depth estimation, a dedicated depth hint module is designed to
generate row-wise features named as depth hints, which are explicitly
supervised in a bin-wise manner. In the training stage, the regression outputs
are uniformly encoded to enable loss disentanglement. The 2D loss term is
further adapted to be depth-aware for improving the detection accuracy of small
objects. The contributions of this work are validated by conducting experiments
and ablation study on the KITTI benchmark. Without utilizing depth priors, post
optimization, or other refinement modules, our network performs competitively
against state-of-the-art methods while maintaining a decent running speed.",['Computer Vision and Pattern Recognition']
Autonomous Learning of Features for Control: Experiments with Embodied and Situated Agents,"As discussed in previous studies, the efficacy of evolutionary or
reinforcement learning algorithms for continuous control optimization can be
enhanced by including a neural module dedicated to feature extraction trained
through self-supervised methods. In this paper we report additional experiments
supporting this hypothesis and we demonstrate how the advantage provided by
feature extraction is not limited to problems that benefit from dimensionality
reduction or that involve agents operating on the basis of allocentric
perception. We introduce a method that permits to continue the training of the
feature-extraction module during the training of the policy network and that
increases the efficacy of feature extraction. Finally, we compare alternative
feature-extracting methods and we show that sequence-to-sequence learning
yields better results than the methods considered in previous studies.","['Machine Learning', 'Artificial Intelligence']"
To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection,"We aim to study the modeling limitations of the commonly employed boosted
decision trees classifier. Inspired by the success of large, data-hungry visual
recognition models (e.g. deep convolutional neural networks), this paper
focuses on the relationship between modeling capacity of the weak learners,
dataset size, and dataset properties. A set of novel experiments on the Caltech
Pedestrian Detection benchmark results in the best known performance among
non-CNN techniques while operating at fast run-time speed. Furthermore, the
performance is on par with deep architectures (9.71% log-average miss rate),
while using only HOG+LUV channels as features. The conclusions from this study
are shown to generalize over different object detection domains as demonstrated
on the FDDB face detection benchmark (93.37% accuracy). Despite the impressive
performance, this study reveals the limited modeling capacity of the common
boosted trees model, motivating a need for architectural changes in order to
compete with multi-level and very deep architectures.",['Computer Vision and Pattern Recognition']
Syntax-Directed Variational Autoencoder for Structured Data,"Deep generative models have been enjoying success in modeling continuous
data. However it remains challenging to capture the representations for
discrete structures with formal grammars and semantics, e.g., computer programs
and molecular structures. How to generate both syntactically and semantically
correct data still remains largely an open problem. Inspired by the theory of
compiler where the syntax and semantics check is done via syntax-directed
translation (SDT), we propose a novel syntax-directed variational autoencoder
(SD-VAE) by introducing stochastic lazy attributes. This approach converts the
offline SDT check into on-the-fly generated guidance for constraining the
decoder. Comparing to the state-of-the-art methods, our approach enforces
constraints on the output space so that the output will be not only
syntactically valid, but also semantically reasonable. We evaluate the proposed
model with applications in programming language and molecules, including
reconstruction and program/molecule optimization. The results demonstrate the
effectiveness in incorporating syntactic and semantic constraints in discrete
generative models, which is significantly better than current state-of-the-art
approaches.","['Machine Learning', 'Computation and Language']"
A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies,"A large number of problems in computer vision can be modelled as energy
minimization problems in a Markov Random Field (MRF) or Conditional Random
Field (CRF) framework. Graph-cuts based $\alpha$-expansion is a standard
move-making method to minimize the energy functions with sub-modular pairwise
terms. However, certain problems require more complex pairwise terms where the
$\alpha$-expansion method is generally not applicable.
  In this paper, we propose an iterative {\em tiered move making algorithm}
which is able to handle general pairwise terms. Each move to the next
configuration is based on the current labeling and an optimal tiered move,
where each tiered move requires one application of the dynamic programming
based tiered labeling method introduced in Felzenszwalb et. al.
\cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimum
for any general pairwise potential, and we give a theoretical analysis of the
properties of the algorithm, characterizing the situations in which we can
expect good performance. We first evaluate our method on an object-class
segmentation problem using the Pascal VOC-11 segmentation dataset where we
learn general pairwise terms. Further we evaluate the algorithm on many other
benchmark labeling problems such as stereo, image segmentation, image stitching
and image denoising. Our method consistently gets better accuracy and energy
values than alpha-expansion, loopy belief propagation (LBP), quadratic
pseudo-boolean optimization (QPBO), and is competitive with TRWS.",['Computer Vision and Pattern Recognition']
Deep Knowledge Tracing with Convolutions,"Knowledge tracing (KT) has recently been an active research area of
computational pedagogy. The task is to model students mastery level of
knowledge based on their responses to the questions in the past, as well as
predict the probabilities that they correctly answer subsequent questions in
the future. A good KT model can not only make students timely aware of their
knowledge states, but also help teachers develop better personalized teaching
plans for students. KT tasks were historically solved using statistical
modeling methods such as Bayesian inference and factor analysis, but recent
advances in deep learning have led to the successive proposals that leverage
deep neural networks, including long short-term memory networks,
memory-augmented networks and self-attention networks. While those deep models
demonstrate superior performance over the traditional approaches, they all
neglect more or less the impact on knowledge states of the most recent
questions answered by students. The forgetting curve theory states that human
memory retention declines over time, therefore knowledge states should be
mostly affected by the recent questions. Based on this observation, we propose
a Convolutional Knowledge Tracing (CKT) model in this paper. In addition to
modeling the long-term effect of the entire question-answer sequence, CKT also
strengthens the short-term effect of recent questions using 3D convolutions,
thereby effectively modeling the forgetting curve in the learning process.
Extensive experiments show that CKT achieves the new state-of-the-art in
predicting students performance compared with existing models. Using CKT, we
gain 1.55 and 2.03 improvements in terms of AUC over DKT and DKVMN
respectively, on the ASSISTments2009 dataset. And on the ASSISTments2015
dataset, the corresponding improvements are 1.01 and 1.96 respectively.","['Machine Learning', 'Artificial Intelligence', 'Computers and Society']"
Characters as Graphs: Recognizing Online Handwritten Chinese Characters via Spatial Graph Convolutional Network,"Chinese is one of the most widely used languages in the world, yet online
handwritten Chinese character recognition (OLHCCR) remains challenging. To
recognize Chinese characters, one popular choice is to adopt the 2D
convolutional neural network (2D-CNN) on the extracted feature images, and
another one is to employ the recurrent neural network (RNN) or 1D-CNN on the
time-series features. Instead of viewing characters as either static images or
temporal trajectories, here we propose to represent characters as geometric
graphs, retaining both spatial structures and temporal orders. Accordingly, we
propose a novel spatial graph convolution network (SGCN) to effectively
classify those character graphs for the first time. Specifically, our SGCN
incorporates the local neighbourhood information via spatial graph convolutions
and further learns the global shape properties with a hierarchical residual
structure. Experiments on IAHCC-UCAS2016, ICDAR-2013, and UNIPEN datasets
demonstrate that the SGCN can achieve comparable recognition performance with
the state-of-the-art methods for character recognition.",['Computer Vision and Pattern Recognition']
Learning from Counting: Leveraging Temporal Classification for Weakly Supervised Object Localization and Detection,"This paper reports a new solution of leveraging temporal classification to
support weakly supervised object detection (WSOD). Specifically, we introduce
raster scan-order techniques to serialize 2D images into 1D sequence data, and
then leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist
Temporal Classification) network to achieve object localization based on a
total count (of interested objects). We term our proposed network LSTM-CCTC
(Count-based CTC). This ""learning from counting"" strategy differs from existing
WSOD methods in that our approach automatically identifies critical points on
or near a target object. This strategy significantly reduces the need of
generating a large number of candidate proposals for object localization.
Experiments show that our method yields state-of-the-art performance based on
an evaluation on PASCAL VOC datasets.",['Computer Vision and Pattern Recognition']
On the spatial attention in Spatio-Temporal Graph Convolutional Networks for skeleton-based human action recognition,"Graph convolutional networks (GCNs) achieved promising performance in
skeleton-based human action recognition by modeling a sequence of skeletons as
a spatio-temporal graph. Most of the recently proposed GCN-based methods
improve the performance by learning the graph structure at each layer of the
network using a spatial attention applied on a predefined graph Adjacency
matrix that is optimized jointly with model's parameters in an end-to-end
manner. In this paper, we analyze the spatial attention used in spatio-temporal
GCN layers and propose a symmetric spatial attention for better reflecting the
symmetric property of the relative positions of the human body joints when
executing actions. We also highlight the connection of spatio-temporal GCN
layers employing additive spatial attention to bilinear layers, and we propose
the spatio-temporal bilinear network (ST-BLN) which does not require the use of
predefined Adjacency matrices and allows for more flexible design of the model.
Experimental results show that the three models lead to effectively the same
performance. Moreover, by exploiting the flexibility provided by the proposed
ST-BLN, one can increase the efficiency of the model.",['Computer Vision and Pattern Recognition']
Efficient piecewise training of deep structured models for semantic segmentation,"Recent advances in semantic image segmentation have mostly been achieved by
training deep convolutional neural networks (CNNs). We show how to improve
semantic segmentation through the use of contextual information; specifically,
we explore `patch-patch' context between image regions, and `patch-background'
context. For learning from the patch-patch context, we formulate Conditional
Random Fields (CRFs) with CNN-based pairwise potential functions to capture
semantic correlations between neighboring patches. Efficient piecewise training
of the proposed deep structured model is then applied to avoid repeated
expensive CRF inference for back propagation. For capturing the
patch-background context, we show that a network design with traditional
multi-scale image input and sliding pyramid pooling is effective for improving
performance. Our experimental results set new state-of-the-art performance on a
number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC
2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an
intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012
dataset.",['Computer Vision and Pattern Recognition']
Tracking system of Mine Patrol Robot for Low Illumination Environment,"Computer vision has received a significant attention in recent years, which
is one of the important parts for robots to apperceive external environment.
Discriminative Correlation Filter (DCF) based trackers gained more popularity
due to their efficiency, however, tracking in low-illumination environments is
a challenging problem, not yet successfully addressed in the literature. In
this work, we tackle the problems by introducing Low-Illumination Long-term
Correlation Tracker (LLCT). First, fused features only including HOG and Color
Names are employed to boost the tracking efficiency. Second, we used the
standard PCA to reduction scheme in the translation and scale estimation phase
for accelerating. Third, we learned a long-term correlation filter to keep the
long-term memory ability. Finally, update memory templates with interval
updates, then re-match existing and initial templates every few frames to
maintain template accuracy. The extensive experiments on popular Object
Tracking Benchmark OTB-50 datasets have demonstrated that the proposed tracker
outperforms the state-of-the-art trackers significantly achieves a high
real-time (33FPS) performance. In addition, the proposed approach can be
integrated easily in robot system and the running speed performed well. The
experimental results show that the novel tracker performance in
low-illumination environment is better than that of general trackers.",['Computer Vision and Pattern Recognition']
Reinforcement Learning for Learning Rate Control,"Stochastic gradient descent (SGD), which updates the model parameters by
adding a local gradient times a learning rate at each step, is widely used in
model training of machine learning algorithms such as neural networks. It is
observed that the models trained by SGD are sensitive to learning rates and
good learning rates are problem specific. We propose an algorithm to
automatically learn learning rates using neural network based actor-critic
methods from deep reinforcement learning (RL).In particular, we train a policy
network called actor to decide the learning rate at each step during training,
and a value network called critic to give feedback about quality of the
decision (e.g., the goodness of the learning rate outputted by the actor) that
the actor made. The introduction of auxiliary actor and critic networks helps
the main network achieve better performance. Experiments on different datasets
and network architectures show that our approach leads to better convergence of
SGD than human-designed competitors.",['Machine Learning']
Conditional MoCoGAN for Zero-Shot Video Generation,"We propose a conditional generative adversarial network (GAN) model for
zero-shot video generation. In this study, we have explored zero-shot
conditional generation setting. In other words, we generate unseen videos from
training samples with missing classes. The task is an extension of conditional
data generation. The key idea is to learn disentangled representations in the
latent space of a GAN. To realize this objective, we base our model on the
motion and content decomposed GAN and conditional GAN for image generation. We
build the model to find better-disentangled representations and to generate
good-quality videos. We demonstrate the effectiveness of our proposed model
through experiments on the Weizmann action database and the MUG facial
expression database.",['Computer Vision and Pattern Recognition']
PhishGAN: Data Augmentation and Identification of Homoglpyh Attacks,"Homoglyph attacks are a common technique used by hackers to conduct phishing.
Domain names or links that are visually similar to actual ones are created via
punycode to obfuscate the attack, making the victim more susceptible to
phishing. For example, victims may mistake ""|inkedin.com"" for ""linkedin.com""
and in the process, divulge personal details to the fake website. Current State
of The Art (SOTA) typically make use of string comparison algorithms (e.g.
Levenshtein Distance), which are computationally heavy. One reason for this is
the lack of publicly available datasets thus hindering the training of more
advanced Machine Learning (ML) models. Furthermore, no one font is able to
render all types of punycode correctly, posing a significant challenge to the
creation of a dataset that is unbiased toward any particular font. This coupled
with the vast number of internet domains pose a challenge in creating a dataset
that can capture all possible variations. Here, we show how a conditional
Generative Adversarial Network (GAN), PhishGAN, can be used to generate images
of hieroglyphs, conditioned on non-homoglpyh input text images. Practical
changes to current SOTA were required to facilitate the generation of more
varied homoglyph text-based images. We also demonstrate a workflow of how
PhishGAN together with a Homoglyph Identifier (HI) model can be used to
identify the domain the homoglyph was trying to imitate. Furthermore, we
demonstrate how PhishGAN's ability to generate datasets on the fly facilitate
the quick adaptation of cybersecurity systems to detect new threats as they
emerge.","['Computer Vision and Pattern Recognition', 'Cryptography and Security', 'Machine Learning']"
Hierarchy Parsing for Image Captioning,"It is always well believed that parsing an image into constituent visual
patterns would be helpful for understanding and representing an image.
Nevertheless, there has not been evidence in support of the idea on describing
an image with a natural-language utterance. In this paper, we introduce a new
design to model a hierarchy from instance level (segmentation), region level
(detection) to the whole image to delve into a thorough image understanding for
captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture
that novelly integrates hierarchical structure into image encoder. Technically,
an image decomposes into a set of regions and some of the regions are resolved
into finer ones. Each region then regresses to an instance, i.e., foreground of
the region. Such process naturally builds a hierarchal tree. A tree-structured
Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the
hierarchal structure and enhance all the instance-level, region-level and
image-level features. Our HIP is appealing in view that it is pluggable to any
neural captioning models. Extensive experiments on COCO image captioning
dataset demonstrate the superiority of HIP. More remarkably, HIP plus a
top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1%
to 127.2% on COCO Karpathy test split. When further endowing instance-level and
region-level features from HIP with semantic relation learnt through Graph
Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.","['Computer Vision and Pattern Recognition', 'Computation and Language']"
Residual Non-local Attention Networks for Image Restoration,"In this paper, we propose a residual non-local attention network for
high-quality image restoration. Without considering the uneven distribution of
information in the corrupted images, previous methods are restricted by local
convolutional operation and equal treatment of spatial- and channel-wise
features. To address this issue, we design local and non-local attention blocks
to extract features that capture the long-range dependencies between pixels and
pay more attention to the challenging parts. Specifically, we design trunk
branch and (non-)local mask branch in each (non-)local attention block. The
trunk branch is used to extract hierarchical features. Local and non-local mask
branches aim to adaptively rescale these hierarchical features with mixed
attentions. The local mask branch concentrates on more local structures with
convolutional operations, while non-local attention considers more about
long-range dependencies in the whole feature map. Furthermore, we propose
residual local and non-local attention learning to train the very deep network,
which further enhance the representation ability of the network. Our proposed
method can be generalized for various image restoration applications, such as
image denoising, demosaicing, compression artifacts reduction, and
super-resolution. Experiments demonstrate that our method obtains comparable or
better results compared with recently leading methods quantitatively and
visually.",['Computer Vision and Pattern Recognition']
Contextualized Keyword Representations for Multi-modal Retinal Image Captioning,"Medical image captioning automatically generates a medical description to
describe the content of a given medical image. A traditional medical image
captioning model creates a medical description only based on a single medical
image input. Hence, an abstract medical description or concept is hard to be
generated based on the traditional approach. Such a method limits the
effectiveness of medical image captioning. Multi-modal medical image captioning
is one of the approaches utilized to address this problem. In multi-modal
medical image captioning, textual input, e.g., expert-defined keywords, is
considered as one of the main drivers of medical description generation. Thus,
encoding the textual input and the medical image effectively are both important
for the task of multi-modal medical image captioning. In this work, a new
end-to-end deep multi-modal medical image captioning model is proposed.
Contextualized keyword representations, textual feature reinforcement, and
masked self-attention are used to develop the proposed approach. Based on the
evaluation of the existing multi-modal medical image captioning dataset,
experimental results show that the proposed model is effective with the
increase of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the
state-of-the-art method.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Computation and Language', 'Information Retrieval', 'Multimedia']"
Binary Graph Neural Networks,"Graph Neural Networks (GNNs) have emerged as a powerful and flexible
framework for representation learning on irregular data. As they generalize the
operations of classical CNNs on grids to arbitrary topologies, GNNs also bring
much of the implementation challenges of their Euclidean counterparts. Model
size, memory footprint, and energy consumption are common concerns for many
real-world applications. Network binarization allocates a single bit to
parameters and activations, thus dramatically reducing the memory requirements
(up to 32x compared to single-precision floating-point numbers) and maximizing
the benefits of fast SIMD instructions on modern hardware for measurable
speedups. However, in spite of the large body of work on binarization for
classical CNNs, this area remains largely unexplored in geometric deep
learning. In this paper, we present and evaluate different strategies for the
binarization of graph neural networks. We show that through careful design of
the models, and control of the training process, binary graph neural networks
can be trained at only a moderate cost in accuracy on challenging benchmarks.
In particular, we present the first dynamic graph neural network in Hamming
space, able to leverage efficient k-NN search on binary vectors to speed-up the
construction of the dynamic graph. We further verify that the binary models
offer significant savings on embedded devices. Our code is publicly available
on Github.","['Machine Learning', 'Artificial Intelligence', 'Computer Vision and Pattern Recognition']"
CLEVRER: CoLlision Events for Video REpresentation and Reasoning,"The ability to reason about temporal and causal events from videos lies at
the core of human intelligence. Most video reasoning benchmarks, however, focus
on pattern recognition from complex visual and language input, instead of on
causal structure. We study the complementary problem, exploring the temporal
and causal structures behind videos of objects with simple visual appearance.
To this end, we introduce the CoLlision Events for Video REpresentation and
Reasoning (CLEVRER), a diagnostic video dataset for systematic evaluation of
computational models on a wide range of reasoning tasks. Motivated by the
theory of human casual judgment, CLEVRER includes four types of questions:
descriptive (e.g., ""what color""), explanatory (""what is responsible for""),
predictive (""what will happen next""), and counterfactual (""what if""). We
evaluate various state-of-the-art models for visual reasoning on our benchmark.
While these models thrive on the perception-based task (descriptive), they
perform poorly on the causal tasks (explanatory, predictive and
counterfactual), suggesting that a principled approach for causal reasoning
should incorporate the capability of both perceiving complex visual and
language inputs, and understanding the underlying dynamics and causal
relations. We also study an oracle model that explicitly combines these
components via symbolic representations.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Computation and Language', 'Machine Learning']"
Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm,"Object detection when provided image-level labels instead of instance-level
labels (i.e., bounding boxes) during training is an important problem in
computer vision, since large scale image datasets with instance-level labels
are extremely costly to obtain. In this paper, we address this challenging
problem by developing an Expectation-Maximization (EM) based object detection
method using deep convolutional neural networks (CNNs). Our method is
applicable to both the weakly-supervised and semi-supervised settings.
Extensive experiments on PASCAL VOC 2007 benchmark show that (1) in the weakly
supervised setting, our method provides significant detection performance
improvement over current state-of-the-art methods, (2) having access to a small
number of strongly (instance-level) annotated images, our method can almost
match the performace of the fully supervised Fast RCNN. We share our source
code at https://github.com/ZiangYan/EM-WSD.",['Computer Vision and Pattern Recognition']
Structure-Regularized Attention for Deformable Object Representation,"Capturing contextual dependencies has proven useful to improve the
representational power of deep neural networks. Recent approaches that focus on
modeling global context, such as self-attention and non-local operation,
achieve this goal by enabling unconstrained pairwise interactions between
elements. In this work, we consider learning representations for deformable
objects which can benefit from context exploitation by modeling the structural
dependencies that the data intrinsically possesses. To this end, we provide a
novel structure-regularized attention mechanism, which formalizes feature
interaction as structural factorization through the use of a pair of
light-weight operations. The instantiated building blocks can be directly
incorporated into modern convolutional neural networks, to boost the
representational power in an efficient manner. Comprehensive studies on
multiple tasks and empirical comparisons with modern attention mechanisms
demonstrate the gains brought by our method in terms of both performance and
model complexity. We further investigate its effect on feature representations,
showing that our trained models can capture diversified representations
characterizing object parts without resorting to extra supervision.",['Computer Vision and Pattern Recognition']
WSLLN: Weakly Supervised Natural Language Localization Networks,"We propose weakly supervised language localization networks (WSLLN) to detect
events in long, untrimmed videos given language queries. To learn the
correspondence between visual segments and texts, most previous methods require
temporal coordinates (start and end times) of events for training, which leads
to high costs of annotation. WSLLN relieves the annotation burden by training
with only video-sentence pairs without accessing to temporal locations of
events. With a simple end-to-end structure, WSLLN measures segment-text
consistency and conducts segment selection (conditioned on the text)
simultaneously. Results from both are merged and optimized as a video-sentence
matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate
that WSLLN achieves state-of-the-art performance.",['Computer Vision and Pattern Recognition']
"Past, Present, and Future Approaches Using Computer Vision for Animal Re-Identification from Camera Trap Data","The ability of a researcher to re-identify (re-ID) an individual animal upon
re-encounter is fundamental for addressing a broad range of questions in the
study of ecosystem function, community and population dynamics, and behavioural
ecology. In this review, we describe a brief history of camera traps for re-ID,
present a collection of computer vision feature engineering methodologies
previously used for animal re-ID, provide an introduction to the underlying
mechanisms of deep learning relevant to animal re-ID, highlight the success of
deep learning methods for human re-ID, describe the few ecological studies
currently utilizing deep learning for camera trap analyses, and our predictions
for near future methodologies based on the rapid development of deep learning
methods. By utilizing novel deep learning methods for object detection and
similarity comparisons, ecologists can extract animals from an image/video data
and train deep learning classifiers to re-ID animal individuals beyond the
capabilities of a human observer. This methodology will allow ecologists with
camera/video trap data to re-identify individuals that exit and re-enter the
camera frame. Our expectation is that this is just the beginning of a major
trend that could stand to revolutionize the analysis of camera trap data and,
ultimately, our approach to animal ecology.",['Computer Vision and Pattern Recognition']
Liver Lesion Detection from Weakly-labeled Multi-phase CT Volumes with a Grouped Single Shot MultiBox Detector,"We present a focal liver lesion detection model leveraged by custom-designed
multi-phase computed tomography (CT) volumes, which reflects real-world
clinical lesion detection practice using a Single Shot MultiBox Detector (SSD).
We show that grouped convolutions effectively harness richer information of the
multi-phase data for the object detection model, while a naive application of
SSD suffers from a generalization gap. We trained and evaluated the modified
SSD model and recently proposed variants with our CT dataset of 64 subjects by
five-fold cross validation. Our model achieved a 53.3% average precision score
and ran in under three seconds per volume, outperforming the original model and
state-of-the-art variants. Results show that the one-stage object detection
model is a practical solution, which runs in near real-time and can learn an
unbiased feature representation from a large-volume real-world detection
dataset, which requires less tedious and time consuming construction of the
weak phase-level bounding box labels.",['Computer Vision and Pattern Recognition']
Automated Quality Control in Image Segmentation: Application to the UK Biobank Cardiac MR Imaging Study,"Background: The trend towards large-scale studies including population
imaging poses new challenges in terms of quality control (QC). This is a
particular issue when automatic processing tools, e.g. image segmentation
methods, are employed to derive quantitative measures or biomarkers for later
analyses. Manual inspection and visual QC of each segmentation isn't feasible
at large scale. However, it's important to be able to automatically detect when
a segmentation method fails so as to avoid inclusion of wrong measurements into
subsequent analyses which could lead to incorrect conclusions. Methods: To
overcome this challenge, we explore an approach for predicting segmentation
quality based on Reverse Classification Accuracy, which enables us to
discriminate between successful and failed segmentations on a per-cases basis.
We validate this approach on a new, large-scale manually-annotated set of 4,800
cardiac magnetic resonance scans. We then apply our method to a large cohort of
7,250 cardiac MRI on which we have performed manual QC. Results: We report
results used for predicting segmentation quality metrics including Dice
Similarity Coefficient (DSC) and surface-distance measures. As initial
validation, we present data for 400 scans demonstrating 99% accuracy for
classifying low and high quality segmentations using predicted DSC scores. As
further validation we show high correlation between real and predicted scores
and 95% classification accuracy on 4,800 scans for which manual segmentations
were available. We mimic real-world application of the method on 7,250 cardiac
MRI where we show good agreement between predicted quality metrics and manual
visual QC scores. Conclusions: We show that RCA has the potential for accurate
and fully automatic segmentation QC on a per-case basis in the context of
large-scale population imaging as in the UK Biobank Imaging Study.",['Computer Vision and Pattern Recognition']
Improving Clinical Outcome Predictions Using Convolution over Medical Entities with Multimodal Learning,"Early prediction of mortality and length of stay(LOS) of a patient is vital
for saving a patient's life and management of hospital resources. Availability
of electronic health records(EHR) makes a huge impact on the healthcare domain
and there has seen several works on predicting clinical problems. However, many
studies did not benefit from the clinical notes because of the sparse, and high
dimensional nature. In this work, we extract medical entities from clinical
notes and use them as additional features besides time-series features to
improve our predictions. We propose a convolution based multimodal
architecture, which not only learns effectively combining medical entities and
time-series ICU signals of patients, but also allows us to compare the effect
of different embedding techniques such as Word2vec, FastText on medical
entities. In the experiments, our proposed method robustly outperforms all
other baseline models including different multimodal architectures for all
clinical tasks. The code for the proposed method is available at
https://github.com/tanlab/ConvolutionMedicalNer.","['Machine Learning', 'Computation and Language', 'Information Retrieval']"
Modeling Object Dissimilarity for Deep Saliency Prediction,"Saliency prediction has made great strides over the past two decades, with
current techniques modeling low-level information, such as color, intensity and
size contrasts, and high-level one, such as attention and gaze direction for
entire objects. Despite this, these methods fail to account for the
dissimilarity between objects, which humans naturally do. In this paper, we
introduce a detection-guided saliency prediction network that explicitly models
the differences between multiple objects, such as their appearance and size
dissimilarities. Our approach is general, allowing us to fuse our object
dissimilarities with features extracted by any deep saliency prediction
network. As evidenced by our experiments, this consistently boosts the accuracy
of the baseline networks, enabling us to outperform the state-of-the-art models
on three saliency benchmarks, namely SALICON, MIT300 and CAT2000.",['Computer Vision and Pattern Recognition']
Linguistic Structure Guided Context Modeling for Referring Image Segmentation,"Referring image segmentation aims to predict the foreground mask of the
object referred by a natural language sentence. Multimodal context of the
sentence is crucial to distinguish the referent from the background. Existing
methods either insufficiently or redundantly model the multimodal context. To
tackle this problem, we propose a ""gather-propagate-distribute"" scheme to model
multimodal context by cross-modal interaction and implement this scheme as a
novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM
module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which
guides all the words to include valid multimodal context of the sentence while
excluding disturbing ones through three steps over the multimodal feature,
i.e., gathering, constrained propagation and distributing. Extensive
experiments on four benchmarks demonstrate that our method outperforms all the
previous state-of-the-arts.","['Computer Vision and Pattern Recognition', 'Computation and Language']"
Improved and efficient inter-vehicle distance estimation using road gradients of both ego and target vehicles,"In advanced driver assistant systems and autonomous driving, it is crucial to
estimate distances between an ego vehicle and target vehicles. Existing
inter-vehicle distance estimation methods assume that the ego and target
vehicles drive on a same ground plane. In practical driving environments,
however, they may drive on different ground planes. This paper proposes an
inter-vehicle distance estimation framework that can consider slope changes of
a road forward, by estimating road gradients of \emph{both} ego vehicle and
target vehicles and using a 2D object detection deep net. Numerical experiments
demonstrate that the proposed method significantly improves the distance
estimation accuracy and time complexity, compared to deep learning-based depth
estimation methods.",['Computer Vision and Pattern Recognition']
Bandwidth selection for kernel estimation in mixed multi-dimensional spaces,"Kernel estimation techniques, such as mean shift, suffer from one major
drawback: the kernel bandwidth selection. The bandwidth can be fixed for all
the data set or can vary at each points. Automatic bandwidth selection becomes
a real challenge in case of multidimensional heterogeneous features. This paper
presents a solution to this problem. It is an extension of \cite{Comaniciu03a}
which was based on the fundamental property of normal distributions regarding
the bias of the normalized density gradient. The selection is done iteratively
for each type of features, by looking for the stability of local bandwidth
estimates across a predefined range of bandwidths. A pseudo balloon mean shift
filtering and partitioning are introduced. The validity of the method is
demonstrated in the context of color image segmentation based on a
5-dimensional space.",['Computer Vision and Pattern Recognition']
Texture Synthesis Guided Deep Hashing for Texture Image Retrieval,"With the large-scale explosion of images and videos over the internet,
efficient hashing methods have been developed to facilitate memory and time
efficient retrieval of similar images. However, none of the existing works uses
hashing to address texture image retrieval mostly because of the lack of
sufficiently large texture image databases. Our work addresses this problem by
developing a novel deep learning architecture that generates binary hash codes
for input texture images. For this, we first pre-train a Texture Synthesis
Network (TSN) which takes a texture patch as input and outputs an enlarged view
of the texture by injecting newer texture content. Thus it signifies that the
TSN encodes the learnt texture specific information in its intermediate layers.
In the next stage, a second network gathers the multi-scale feature
representations from the TSN's intermediate layers using channel-wise
attention, combines them in a progressive manner to a dense continuous
representation which is finally converted into a binary hash code with the help
of individual and pairwise label information. The new enlarged texture patches
also help in data augmentation to alleviate the problem of insufficient texture
data and are used to train the second stage of the network. Experiments on
three public texture image retrieval datasets indicate the superiority of our
texture synthesis guided hashing approach over current state-of-the-art
methods.",['Computer Vision and Pattern Recognition']
Towards Stable Symbol Grounding with Zero-Suppressed State AutoEncoder,"While classical planning has been an active branch of AI, its applicability
is limited to the tasks precisely modeled by humans. Fully automated high-level
agents should be instead able to find a symbolic representation of an unknown
environment without supervision, otherwise it exhibits the knowledge
acquisition bottleneck. Meanwhile, Latplan (Asai and Fukunaga 2018) partially
resolves the bottleneck with a neural network called State AutoEncoder (SAE).
SAE obtains the propositional representation of the image-based puzzle domains
with unsupervised learning, generates a state space and performs classical
planning. In this paper, we identify the problematic, stochastic behavior of
the SAE-produced propositions as a new sub-problem of symbol grounding problem,
the symbol stability problem. Informally, symbols are stable when their
referents (e.g. propositional values) do not change against small perturbation
of the observation, and unstable symbols are harmful for symbolic reasoning. We
analyze the problem in Latplan both formally and empirically, and propose
""Zero-Suppressed SAE"", an enhancement that stabilizes the propositions using
the idea of closed-world assumption as a prior for NN optimization. We show
that it finds the more stable propositions and the more compact
representations, resulting in an improved success rate of Latplan. It is robust
against various hyperparameters and eases the tuning effort, and also provides
a weight pruning capability as a side effect.","['Machine Learning', 'Artificial Intelligence']"
Relational Embedding for Few-Shot Classification,"We propose to address the problem of few-shot classification by meta-learning
""what to observe"" and ""where to attend"" in a relational perspective. Our method
leverages relational patterns within and between images via self-correlational
representation (SCR) and cross-correlational attention (CCA). Within each
image, the SCR module transforms a base feature map into a self-correlation
tensor and learns to extract structural patterns from the tensor. Between the
images, the CCA module computes cross-correlation between two image
representations and learns to produce co-attention between them. Our Relational
Embedding Network (RENet) combines the two relational modules to learn
relational embedding in an end-to-end manner. In experimental evaluation, it
achieves consistent improvements over state-of-the-art methods on four widely
used few-shot classification benchmarks of miniImageNet, tieredImageNet,
CUB-200-2011, and CIFAR-FS.",['Computer Vision and Pattern Recognition']
Using Generative Adversarial Nets on Atari Games for Feature Extraction in Deep Reinforcement Learning,"Deep Reinforcement Learning (DRL) has been successfully applied in several
research domains such as robot navigation and automated video game playing.
However, these methods require excessive computation and interaction with the
environment, so enhancements on sample efficiency are required. The main reason
for this requirement is that sparse and delayed rewards do not provide an
effective supervision for representation learning of deep neural networks. In
this study, Proximal Policy Optimization (PPO) algorithm is augmented with
Generative Adversarial Networks (GANs) to increase the sample efficiency by
enforcing the network to learn efficient representations without depending on
sparse and delayed rewards as supervision. The results show that an increased
performance can be obtained by jointly training a DRL agent with a GAN
discriminator.
  ----
  Derin Pekistirmeli Ogrenme, robot navigasyonu ve otomatiklestirilmis video
oyunu oynama gibi arastirma alanlarinda basariyla uygulanmaktadir. Ancak,
kullanilan yontemler ortam ile fazla miktarda etkilesim ve hesaplama
gerektirmekte ve bu nedenle de ornek verimliligi yonunden iyilestirmelere
ihtiyac duyulmaktadir. Bu gereksinimin en onemli nedeni, gecikmeli ve seyrek
odul sinyallerinin derin yapay sinir aglarinin etkili betimlemeler
ogrenebilmesi icin yeterli bir denetim saglayamamasidir. Bu calismada,
Proksimal Politika Optimizasyonu algoritmasi Uretici Cekismeli Aglar (UCA) ile
desteklenerek derin yapay sinir aglarinin seyrek ve gecikmeli odul sinyallerine
bagimli olmaksizin etkili betimlemeler ogrenmesi tesvik edilmektedir. Elde
edilen sonuclar onerilen algoritmanin ornek verimliliginde artis elde ettigini
gostermektedir.","['Machine Learning', 'Artificial Intelligence']"
Combining Background Subtraction Algorithms with Convolutional Neural Network,"Accurate and fast extraction of foreground object is a key prerequisite for a
wide range of computer vision applications such as object tracking and
recognition. Thus, enormous background subtraction methods for foreground
object detection have been proposed in recent decades. However, it is still
regarded as a tough problem due to a variety of challenges such as illumination
variations, camera jitter, dynamic backgrounds, shadows, and so on. Currently,
there is no single method that can handle all the challenges in a robust way.
In this letter, we try to solve this problem from a new perspective by
combining different state-of-the-art background subtraction algorithms to
create a more robust and more advanced foreground detection algorithm. More
specifically, an encoder-decoder fully convolutional neural network
architecture is trained to automatically learn how to leverage the
characteristics of different algorithms to fuse the results produced by
different background subtraction algorithms and output a more precise result.
Comprehensive experiments evaluated on the CDnet 2014 dataset demonstrate that
the proposed method outperforms all the considered single background
subtraction algorithm. And we show that our solution is more efficient than
other combination strategies.",['Computer Vision and Pattern Recognition']
Show Why the Answer is Correct! Towards Explainable AI using Compositional Temporal Attention,"Visual Question Answering (VQA) models have achieved significant success in
recent times. Despite the success of VQA models, they are mostly black-box
models providing no reasoning about the predicted answer, thus raising
questions for their applicability in safety-critical such as autonomous systems
and cyber-security. Current state of the art fail to better complex questions
and thus are unable to exploit compositionality. To minimize the black-box
effect of these models and also to make them better exploit compositionality,
we propose a Dynamic Neural Network (DMN), which can understand a particular
question and then dynamically assemble various relatively shallow deep learning
modules from a pool of modules to form a network. We incorporate compositional
temporal attention to these deep learning based modules to increase
compositionality exploitation. This results in achieving better understanding
of complex questions and also provides reasoning as to why the module predicts
a particular answer. Experimental analysis on the two benchmark datasets,
VQA2.0 and CLEVR, depicts that our model outperforms the previous approaches
for Visual Question Answering task as well as provides better reasoning, thus
making it reliable for mission critical applications like safety and security.",['Computer Vision and Pattern Recognition']
Improved RAMEN: Towards Domain Generalization for Visual Question Answering,"Currently nearing human-level performance, Visual Question Answering (VQA) is
an emerging area in artificial intelligence.
  Established as a multi-disciplinary field in machine learning, both computer
vision and natural language processing communities are working together to
achieve state-of-the-art (SOTA) performance.
  However, there is a gap between the SOTA results and real world applications.
  This is due to the lack of model generalisation.
  The RAMEN model \cite{Shrestha2019} aimed to achieve domain generalization by
obtaining the highest score across two main types of VQA datasets.
  This study provides two major improvements to the early/late fusion module
and aggregation module of the RAMEN architecture, with the objective of further
strengthening domain generalization.
  Vector operations based fusion strategies are introduced for the fusion
module and the transformer architecture is introduced for the aggregation
module.
  Improvements of up to five VQA datasets from the experiments conducted are
evident.
  Following the results, this study analyses the effects of both the
improvements on the domain generalization problem.
  The code is available on GitHub though the following link
\url{https://github.com/bhanukaManesha/ramen}.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction,"Human motion prediction is a challenging task due to the stochasticity and
aperiodicity of future poses. Recently, graph convolutional network has been
proven to be very effective to learn dynamic relations among pose joints, which
is helpful for pose prediction. On the other hand, one can abstract a human
pose recursively to obtain a set of poses at multiple scales. With the increase
of the abstraction level, the motion of the pose becomes more stable, which
benefits pose prediction too. In this paper, we propose a novel Multi-Scale
Residual Graph Convolution Network (MSR-GCN) for human pose prediction task in
the manner of end-to-end. The GCNs are used to extract features from fine to
coarse scale and then from coarse to fine scale. The extracted features at each
scale are then combined and decoded to obtain the residuals between the input
and target poses. Intermediate supervisions are imposed on all the predicted
poses, which enforces the network to learn more representative features. Our
proposed approach is evaluated on two standard benchmark datasets, i.e., the
Human3.6M dataset and the CMU Mocap dataset. Experimental results demonstrate
that our method outperforms the state-of-the-art approaches. Code and
pre-trained models are available at https://github.com/Droliven/MSRGCN.",['Computer Vision and Pattern Recognition']
Joint Visual Grounding with Language Scene Graphs,"Visual grounding is a task to ground referring expressions in images, e.g.,
localize ""the white truck in front of the yellow one"". To resolve this task
fundamentally, the model should first find out the contextual objects (e.g.,
the ""yellow"" truck) and then exploit them to disambiguate the referent from
other similar objects by using the attributes and relationships (e.g., ""white"",
""yellow"", ""in front of""). However, due to the lack of annotations on contextual
objects and their relationships, existing methods degenerate the above joint
grounding process into a holistic association between the expression and
regions, thus suffering from unsatisfactory performance and limited
interpretability. In this paper, we alleviate the missing-annotation problem
and enable the joint reasoning by leveraging the language scene graph which
covers both labeled referent and unlabeled contexts (other objects, attributes,
and relationships). Specifically, the language scene graph is a graphical
representation where the nodes are objects with attributes and the edges are
relationships. We construct a factor graph based on it and then perform
marginalization over the graph, such that we can ground both referent and
contexts on corresponding image regions to achieve the joint visual grounding
(JVG). Experimental results demonstrate that the proposed approach is effective
and interpretable, e.g., on three benchmarks, it outperforms the
state-of-the-art methods while offers a complete grounding of all the objects
mentioned in the referring expression.",['Computer Vision and Pattern Recognition']
Semantic Part Segmentation using Compositional Model combining Shape and Appearance,"In this paper, we study the problem of semantic part segmentation for
animals. This is more challenging than standard object detection, object
segmentation and pose estimation tasks because semantic parts of animals often
have similar appearance and highly varying shapes. To tackle these challenges,
we build a mixture of compositional models to represent the object boundary and
the boundaries of semantic parts. And we incorporate edge, appearance, and
semantic part cues into the compositional model. Given part-level segmentation
annotation, we develop a novel algorithm to learn a mixture of compositional
models under various poses and viewpoints for certain animal classes.
Furthermore, a linear complexity algorithm is offered for efficient inference
of the compositional model using dynamic programming. We evaluate our method
for horse and cow using a newly annotated dataset on Pascal VOC 2010 which has
pixelwise part labels. Experimental results demonstrate the effectiveness of
our method.",['Computer Vision and Pattern Recognition']
Visual Explanation for Deep Metric Learning,"This work explores the visual explanation for deep metric learning and its
applications. As an important problem for learning representation, metric
learning has attracted much attention recently, while the interpretation of
such model is not as well studied as classification. To this end, we propose an
intuitive idea to show where contributes the most to the overall similarity of
two input images by decomposing the final activation. Instead of only providing
the overall activation map of each image, we propose to generate point-to-point
activation intensity between two images so that the relationship between
different regions is uncovered. We show that the proposed framework can be
directly deployed to a large range of metric learning applications and provides
valuable information for understanding the model. Furthermore, our experiments
show its effectiveness on two potential applications, i.e. cross-view pattern
discovery and interactive retrieval. The source code is available at
\url{https://github.com/Jeff-Zilence/Explain_Metric_Learning}.",['Computer Vision and Pattern Recognition']
End-to-End 3D Multi-Object Tracking and Trajectory Forecasting,"3D multi-object tracking (MOT) and trajectory forecasting are two critical
components in modern 3D perception systems. We hypothesize that it is
beneficial to unify both tasks under one framework to learn a shared feature
representation of agent interaction. To evaluate this hypothesis, we propose a
unified solution for 3D MOT and trajectory forecasting which also incorporates
two additional novel computational units. First, we employ a feature
interaction technique by introducing Graph Neural Networks (GNNs) to capture
the way in which multiple agents interact with one another. The GNN is able to
model complex hierarchical interactions, improve the discriminative feature
learning for MOT association, and provide socially-aware context for trajectory
forecasting. Second, we use a diversity sampling function to improve the
quality and diversity of our forecasted trajectories. The learned sampling
function is trained to efficiently extract a variety of outcomes from a
generative trajectory distribution and helps avoid the problem of generating
many duplicate trajectory samples. We show that our method achieves
state-of-the-art performance on the KITTI dataset. Our project website is at
http://www.xinshuoweng.com/projects/GNNTrkForecast.","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Multiagent Systems', 'Robotics']"
Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation,"Vision-language navigation (VLN) is the task of navigating an embodied agent
to carry out natural language instructions inside real 3D environments. In this
paper, we study how to address three critical challenges for this task: the
cross-modal grounding, the ill-posed feedback, and the generalization problems.
First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that
enforces cross-modal grounding both locally and globally via reinforcement
learning (RL). Particularly, a matching critic is used to provide an intrinsic
reward to encourage global matching between instructions and trajectories, and
a reasoning navigator is employed to perform cross-modal grounding in the local
visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model
significantly outperforms previous methods by 10% on SPL and achieves the new
state-of-the-art performance. To improve the generalizability of the learned
policy, we further introduce a Self-Supervised Imitation Learning (SIL) method
to explore unseen environments by imitating its own past, good decisions. We
demonstrate that SIL can approximate a better and more efficient policy, which
tremendously minimizes the success rate performance gap between seen and unseen
environments (from 30.7% to 11.7%).","['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Computation and Language', 'Robotics']"
Future Localization from an Egocentric Depth Image,"This paper presents a method for future localization: to predict a set of
plausible trajectories of ego-motion given a depth image. We predict paths
avoiding obstacles, between objects, even paths turning around a corner into
space behind objects. As a byproduct of the predicted trajectories of
ego-motion, we discover in the image the empty space occluded by foreground
objects. We use no image based features such as semantic labeling/segmentation
or object detection/recognition for this algorithm. Inspired by proxemics, we
represent the space around a person using an EgoSpace map, akin to an
illustrated tourist map, that measures a likelihood of occlusion at the
egocentric coordinate system. A future trajectory of ego-motion is modeled by a
linear combination of compact trajectory bases allowing us to constrain the
predicted trajectory. We learn the relationship between the EgoSpace map and
trajectory from the EgoMotion dataset providing in-situ measurements of the
future trajectory. A cost function that takes into account partial occlusion
due to foreground objects is minimized to predict a trajectory. This cost
function generates a trajectory that passes through the occluded space, which
allows us to discover the empty space behind the foreground objects. We
quantitatively evaluate our method to show predictive validity and apply to
various real world scenes including walking, shopping, and social interactions.",['Computer Vision and Pattern Recognition']
Unsupervised learning for economic risk evaluation in the context of Covid-19 pandemic,"Justifying draconian measures during the Covid-19 pandemic was difficult not
only because of the restriction of individual rights, but also because of its
economic impact. The objective of this work is to present a machine learning
approach to identify regions that should implement similar health policies. For
that end, we successfully developed a system that gives a notion of economic
impact given the prediction of new incidental cases through unsupervised
learning and time series forecasting. This system was built taking into account
computational restrictions and low maintenance requirements in order to improve
the system's resilience. Finally this system was deployed as part of a web
application for simulation and data analysis of COVID-19, in Colombia,
available at (https://covid19.dis.eafit.edu.co).","['Machine Learning', 'Computers and Society']"
Unsupervised automatic classification of Scanning Electron Microscopy (SEM) images of CD4+ cells with varying extent of HIV virion infection,"Archiving large sets of medical or cell images in digital libraries may
require ordering randomly scattered sets of image data according to specific
criteria, such as the spatial extent of a specific local color or contrast
content that reveals different meaningful states of a physiological structure,
tissue, or cell in a certain order, indicating progression or recession of a
pathology, or the progressive response of a cell structure to treatment. Here
we used a Self Organized Map (SOM)-based, fully automatic and unsupervised,
classification procedure described in our earlier work and applied it to sets
of minimally processed grayscale and/or color processed Scanning Electron
Microscopy (SEM) images of CD4+ T-lymphocytes (so-called helper cells) with
varying extent of HIV virion infection. It is shown that the quantization error
in the SOM output after training permits to scale the spatial magnitude and the
direction of change (+ or -) in local pixel contrast or color across images of
a series with a reliability that exceeds that of any human expert. The
procedure is easily implemented and fast, and represents a promising step
towards low-cost automatic digital image archiving with minimal intervention of
a human operator.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence']"
Interactive Image Generation Using Scene Graphs,"Recent years have witnessed some exciting developments in the domain of
generating images from scene-based text descriptions. These approaches have
primarily focused on generating images from a static text description and are
limited to generating images in a single pass. They are unable to generate an
image interactively based on an incrementally additive text description
(something that is more intuitive and similar to the way we describe an image).
We propose a method to generate an image incrementally based on a sequence of
graphs of scene descriptions (scene-graphs). We propose a recurrent network
architecture that preserves the image content generated in previous steps and
modifies the cumulative image as per the newly provided scene information. Our
model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized
scene graphs along with Generative Adversarial image translation networks to
generate realistic multi-object images without needing any intermediate
supervision during training. We experiment with Coco-Stuff dataset which has
multi-object images along with annotations describing the visual scene and show
that our model significantly outperforms other approaches on the same dataset
in generating visually consistent images for incrementally growing scene
graphs.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']"
ADAPT : Awesome Domain Adaptation Python Toolbox,"ADAPT is an open-source python library providing the implementation of
several domain adaptation methods. The library is suited for scikit-learn
estimator object (object which implement fit and predict methods) and
tensorflow models. Most of the implemented methods are developed in an
estimator agnostic fashion, offering various possibilities adapted to multiple
usage. The library offers three modules corresponding to the three principal
strategies of domain adaptation: (i) feature-based containing methods
performing feature transformation; (ii) instance-based with the implementation
of reweighting techniques and (iii) parameter-based proposing methods to adapt
pre-trained models to novel observations. A full documentation is proposed
online https://adapt-python.github.io/adapt/ with gallery of examples. Besides,
the library presents an high test coverage.",['Machine Learning']
Saliency Maps Generation for Automatic Text Summarization,"Saliency map generation techniques are at the forefront of explainable AI
literature for a broad range of machine learning applications. Our goal is to
question the limits of these approaches on more complex tasks. In this paper we
apply Layer-Wise Relevance Propagation (LRP) to a sequence-to-sequence
attention model trained on a text summarization dataset. We obtain unexpected
saliency maps and discuss the rightfulness of these ""explanations"". We argue
that we need a quantitative way of testing the counterfactual case to judge the
truthfulness of the saliency maps. We suggest a protocol to check the validity
of the importance attributed to the input and show that the saliency maps
obtained sometimes capture the real use of the input features by the network,
and sometimes do not. We use this example to discuss how careful we need to be
when accepting them as explanation.","['Machine Learning', 'Computation and Language']"
CNN based texture synthesize with Semantic segment,"Deep learning algorithm display powerful ability in Computer Vision area, in
recent year, the CNN has been applied to solve problems in the subarea of
Image-generating, which has been widely applied in areas such as photo editing,
image design, computer animation, real-time rendering for large scale of scenes
and for visual effects in movies. However in the texture synthesize procedure.
The state-of-art CNN can not capture the spatial location of texture in image,
lead to significant distortion after texture synthesize, we propose a new way
to generating-image by adding the semantic segment step with deep learning
algorithm as Pre-Processing and analyze the outcome.","['Computer Vision and Pattern Recognition', 'Graphics', 'Machine Learning']"
Time Series Domain Adaptation via Sparse Associative Structure Alignment,"Domain adaptation on time series data is an important but challenging task.
Most of the existing works in this area are based on the learning of the
domain-invariant representation of the data with the help of restrictions like
MMD. However, such extraction of the domain-invariant representation is a
non-trivial task for time series data, due to the complex dependence among the
timestamps. In detail, in the fully dependent time series, a small change of
the time lags or the offsets may lead to difficulty in the domain invariant
extraction. Fortunately, the stability of the causality inspired us to explore
the domain invariant structure of the data. To reduce the difficulty in the
discovery of causal structure, we relax it to the sparse associative structure
and propose a novel sparse associative structure alignment model for domain
adaptation. First, we generate the segment set to exclude the obstacle of
offsets. Second, the intra-variables and inter-variables sparse attention
mechanisms are devised to extract associative structure time-series data with
considering time lags. Finally, the associative structure alignment is used to
guide the transfer of knowledge from the source domain to the target one.
Experimental studies not only verify the good performance of our methods on
three real-world datasets but also provide some insightful discoveries on the
transferred knowledge.",['Machine Learning']
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']"
Provable Convergence of Nesterov Accelerated Method for Over-Parameterized Neural Networks,"Despite the empirical success of deep learning, it still lacks theoretical
understandings to explain why randomly initialized neural network trained by
first-order optimization methods is able to achieve zero training loss, even
though its landscape is non-convex and non-smooth. Recently, there are some
works to demystifies this phenomenon under over-parameterized regime. In this
work, we make further progress on this area by considering a commonly used
momentum optimization algorithm: Nesterov accelerated method (NAG). We analyze
the convergence of NAG for two-layer fully connected neural network with ReLU
activation. Specifically, we prove that the error of NAG converges to zero at a
linear convergence rate $1-\Theta(1/\sqrt{\kappa})$, where $\kappa > 1$ is
determined by the initialization and the architecture of neural network.
Comparing to the rate $1-\Theta(1/\kappa)$ of gradient descent, NAG achieves an
acceleration. Besides, it also validates NAG and Heavy-ball method can achieve
a similar convergence rate.","['Machine Learning', 'Artificial Intelligence']"
VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation,"Recently deep residual learning with residual units for training very deep
neural networks advanced the state-of-the-art performance on 2D image
recognition tasks, e.g., object detection and segmentation. However, how to
fully leverage contextual representations for recognition tasks from volumetric
data has not been well studied, especially in the field of medical image
computing, where a majority of image modalities are in volumetric format. In
this paper we explore the deep residual learning on the task of volumetric
brain segmentation. There are at least two main contributions in our work.
First, we propose a deep voxelwise residual network, referred as VoxResNet,
which borrows the spirit of deep residual learning in 2D image recognition
tasks, and is extended into a 3D variant for handling volumetric data. Second,
an auto-context version of VoxResNet is proposed by seamlessly integrating the
low-level image appearance features, implicit shape information and high-level
context together for further improving the volumetric segmentation performance.
Extensive experiments on the challenging benchmark of brain segmentation from
magnetic resonance (MR) images corroborated the efficacy of our proposed method
in dealing with volumetric data. We believe this work unravels the potential of
3D deep learning to advance the recognition performance on volumetric image
segmentation.",['Computer Vision and Pattern Recognition']
Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects,"We address the novel task of jointly reconstructing the 3D shape, texture,
and motion of an object from a single motion-blurred image. While previous
approaches address the deblurring problem only in the 2D image domain, our
proposed rigorous modeling of all object properties in the 3D domain enables
the correct description of arbitrary object motion. This leads to significantly
better image decomposition and sharper deblurring results. We model the
observed appearance of a motion-blurred object as a combination of the
background and a 3D object with constant translation and rotation. Our method
minimizes a loss on reconstructing the input image via differentiable rendering
with suitable regularizers. This enables estimating the textured 3D mesh of the
blurred object with high fidelity. Our method substantially outperforms
competing approaches on several benchmarks for fast moving objects deblurring.
Qualitative results show that the reconstructed 3D mesh generates high-quality
temporal super-resolution and novel views of the deblurred object.",['Computer Vision and Pattern Recognition']
Working Memory Graphs,"Transformers have increasingly outperformed gated RNNs in obtaining new
state-of-the-art results on supervised tasks involving text sequences. Inspired
by this trend, we study the question of how Transformer-based models can
improve the performance of sequential decision-making agents. We present the
Working Memory Graph (WMG), an agent that employs multi-head self-attention to
reason over a dynamic set of vectors representing observed and recurrent state.
We evaluate WMG in three environments featuring factored observation spaces: a
Pathfinding environment that requires complex reasoning over past observations,
BabyAI gridworld levels that involve variable goals, and Sokoban which
emphasizes future planning. We find that the combination of WMG's
Transformer-based architecture with factored observation spaces leads to
significant gains in learning efficiency compared to baseline architectures
across all tasks. WMG demonstrates how Transformer-based models can
dramatically boost sample efficiency in RL environments for which observations
can be factored.","['Machine Learning', 'Artificial Intelligence', 'Computation and Language']"
Fast LIDAR-based Road Detection Using Fully Convolutional Neural Networks,"In this work, a deep learning approach has been developed to carry out road
detection using only LIDAR data. Starting from an unstructured point cloud,
top-view images encoding several basic statistics such as mean elevation and
density are generated. By considering a top-view representation, road detection
is reduced to a single-scale problem that can be addressed with a simple and
fast fully convolutional neural network (FCN). The FCN is specifically designed
for the task of pixel-wise semantic segmentation by combining a large receptive
field with high-resolution feature maps. The proposed system achieved excellent
performance and it is among the top-performing algorithms on the KITTI road
benchmark. Its fast inference makes it particularly suitable for real-time
applications.",['Computer Vision and Pattern Recognition']
A sequential guiding network with attention for image captioning,"The recent advances of deep learning in both computer vision (CV) and natural
language processing (NLP) provide us a new way of understanding semantics, by
which we can deal with more challenging tasks such as automatic description
generation from natural images. In this challenge, the encoder-decoder
framework has achieved promising performance when a convolutional neural
network (CNN) is used as image encoder and a recurrent neural network (RNN) as
decoder. In this paper, we introduce a sequential guiding network that guides
the decoder during word generation. The new model is an extension of the
encoder-decoder framework with attention that has an additional guiding long
short-term memory (LSTM) and can be trained in an end-to-end manner by using
image/descriptions pairs. We validate our approach by conducting extensive
experiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model
achieves significant improvement comparing to the other state-of-the-art deep
learning models.","['Computer Vision and Pattern Recognition', 'Computation and Language']"
Semantic-guided Encoder Feature Learning for Blurry Boundary Delineation,"Encoder-decoder architectures are widely adopted for medical image
segmentation tasks. With the lateral skip connection, the models can obtain and
fuse both semantic and resolution information in deep layers to achieve more
accurate segmentation performance. However, in many applications (e.g., blurry
boundary images), these models often cannot precisely locate complex boundaries
and segment tiny isolated parts. To solve this challenging problem, we firstly
analyze why simple skip connections are not enough to help accurately locate
indistinct boundaries and argue that it is due to the fuzzy information in the
skip connection provided in the encoder layers. Then we propose a
semantic-guided encoder feature learning strategy to learn both high resolution
and rich semantic encoder features so that we can more accurately locate the
blurry boundaries, which can also enhance the network by selectively learning
discriminative features. Besides, we further propose a soft contour constraint
mechanism to model the blurry boundary detection. Experimental results on real
clinical datasets show that our proposed method can achieve state-of-the-art
segmentation accuracy, especially for the blurry regions. Further analysis also
indicates that our proposed network components indeed contribute to the
improvement of performance. Experiments on additional datasets validate the
generalization ability of our proposed method.",['Computer Vision and Pattern Recognition']
Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions,"Various 3D semantic attributes such as segmentation masks, geometric
features, keypoints, and materials can be encoded as per-point probe functions
on 3D geometries. Given a collection of related 3D shapes, we consider how to
jointly analyze such probe functions over different shapes, and how to discover
common latent structures using a neural network --- even in the absence of any
correspondence information. Our network is trained on point cloud
representations of shape geometry and associated semantic functions on that
point cloud. These functions express a shared semantic understanding of the
shapes but are not coordinated in any way. For example, in a segmentation task,
the functions can be indicator functions of arbitrary sets of shape parts, with
the particular combination involved not known to the network. Our network is
able to produce a small dictionary of basis functions for each shape, a
dictionary whose span includes the semantic functions provided for that shape.
Even though our shapes have independent discretizations and no functional
correspondences are provided, the network is able to generate latent bases, in
a consistent order, that reflect the shared semantic structure among the
shapes. We demonstrate the effectiveness of our technique in various
segmentation and keypoint selection applications.",['Computer Vision and Pattern Recognition']
PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation,"LiDAR sensors can provide dependable 3D spatial information at a low
frequency (around 10Hz) and have been widely applied in the field of autonomous
driving and UAV. However, the camera with a higher frequency (around 20Hz) has
to be decreased so as to match with LiDAR in a multi-sensor system. In this
paper, we propose a novel Pseudo-LiDAR interpolation network (PLIN) to increase
the frequency of LiDAR sensors. PLIN can generate temporally and spatially
high-quality point cloud sequences to match the high frequency of cameras. To
achieve this goal, we design a coarse interpolation stage guided by consecutive
sparse depth maps and motion relationship. We also propose a refined
interpolation stage guided by the realistic scene. Using this coarse-to-fine
cascade structure, our method can progressively perceive multi-modal
information and generate accurate intermediate point clouds. To the best of our
knowledge, this is the first deep framework for Pseudo-LiDAR point cloud
interpolation, which shows appealing applications in navigation systems
equipped with LiDAR and cameras. Experimental results demonstrate that PLIN
achieves promising performance on the KITTI dataset, significantly
outperforming the traditional interpolation method and the state-of-the-art
video interpolation technique.",['Computer Vision and Pattern Recognition']
Adapting Mask-RCNN for Automatic Nucleus Segmentation,"Automatic segmentation of microscopy images is an important task in medical
image processing and analysis. Nucleus detection is an important example of
this task. Mask-RCNN is a recently proposed state-of-the-art algorithm for
object detection, object localization, and object instance segmentation of
natural images. In this paper we demonstrate that Mask-RCNN can be used to
perform highly effective and efficient automatic segmentations of a wide range
of microscopy images of cell nuclei, for a variety of cells acquired under a
variety of conditions.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning,"Actor-critic methods are widely used in offline reinforcement learning
practice, but are not so well-understood theoretically. We propose a new
offline actor-critic algorithm that naturally incorporates the pessimism
principle, leading to several key advantages compared to the state of the art.
The algorithm can operate when the Bellman evaluation operator is closed with
respect to the action value function of the actor's policies; this is a more
general setting than the low-rank MDP model. Despite the added generality, the
procedure is computationally tractable as it involves the solution of a
sequence of second-order programs. We prove an upper bound on the suboptimality
gap of the policy returned by the procedure that depends on the data coverage
of any arbitrary, possibly data dependent comparator policy. The achievable
guarantee is complemented with a minimax lower bound that is matching up to
logarithmic factors.",['Machine Learning']
Attention U-Net: Learning Where to Look for the Pancreas,"We propose a novel attention gate (AG) model for medical imaging that
automatically learns to focus on target structures of varying shapes and sizes.
Models trained with AGs implicitly learn to suppress irrelevant regions in an
input image while highlighting salient features useful for a specific task.
This enables us to eliminate the necessity of using explicit external
tissue/organ localisation modules of cascaded convolutional neural networks
(CNNs). AGs can be easily integrated into standard CNN architectures such as
the U-Net model with minimal computational overhead while increasing the model
sensitivity and prediction accuracy. The proposed Attention U-Net architecture
is evaluated on two large CT abdominal datasets for multi-class image
segmentation. Experimental results show that AGs consistently improve the
prediction performance of U-Net across different datasets and training sizes
while preserving computational efficiency. The code for the proposed
architecture is publicly available.",['Computer Vision and Pattern Recognition']
Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition,"Graph Convolutional Networks (GCNs) have attracted increasing interests for
the task of skeleton-based action recognition. The key lies in the design of
the graph structure, which encodes skeleton topology information. In this
paper, we propose Dynamic GCN, in which a novel convolutional neural network
named Contextencoding Network (CeN) is introduced to learn skeleton topology
automatically. In particular, when learning the dependency between two joints,
contextual features from the rest joints are incorporated in a global manner.
CeN is extremely lightweight yet effective, and can be embedded into a graph
convolutional layer. By stacking multiple CeN-enabled graph convolutional
layers, we build Dynamic GCN. Notably, as a merit of CeN, dynamic graph
topologies are constructed for different input samples as well as graph
convolutional layers of various depths. Besides, three alternative context
modeling architectures are well explored, which may serve as a guideline for
future research on graph topology learning. CeN brings only ~7% extra FLOPs for
the baseline model, and Dynamic GCN achieves better performance with
$2\times$~$4\times$ fewer FLOPs than existing methods. By further combining
static physical body connections and motion modalities, we achieve
state-of-the-art performance on three large-scale benchmarks, namely NTU-RGB+D,
NTU-RGB+D 120 and Skeleton-Kinetics.",['Computer Vision and Pattern Recognition']
Robust Real-Time Pedestrian Detection on Embedded Devices,"Detection of pedestrians on embedded devices, such as those on-board of
robots and drones, has many applications including road intersection
monitoring, security, crowd monitoring and surveillance, to name a few.
However, the problem can be challenging due to continuously-changing camera
viewpoint and varying object appearances as well as the need for lightweight
algorithms suitable for embedded systems. This paper proposes a robust
framework for pedestrian detection in many footages. The framework performs
fine and coarse detections on different image regions and exploits temporal and
spatial characteristics to attain enhanced accuracy and real time performance
on embedded boards. The framework uses the Yolo-v3 object detection [1] as its
backbone detector and runs on the Nvidia Jetson TX2 embedded board, however
other detectors and/or boards can be used as well. The performance of the
framework is demonstrated on two established datasets and its achievement of
the second place in CVPR 2019 Embedded Real-Time Inference (ERTI) Challenge.",['Computer Vision and Pattern Recognition']
Transfer Learning for Material Classification using Convolutional Networks,"Material classification in natural settings is a challenge due to complex
interplay of geometry, reflectance properties, and illumination. Previous work
on material classification relies strongly on hand-engineered features of
visual samples. In this work we use a Convolutional Neural Network (convnet)
that learns descriptive features for the specific task of material recognition.
Specifically, transfer learning from the task of object recognition is
exploited to more effectively train good features for material classification.
The approach of transfer learning using convnets yields significantly higher
recognition rates when compared to previous state-of-the-art approaches. We
then analyze the relative contribution of reflectance and shading information
by a decomposition of the image into its intrinsic components. The use of
convnets for material classification was hindered by the strong demand for
sufficient and diverse training data, even with transfer learning approaches.
Therefore, we present a new data set containing approximately 10k images
divided into 10 material categories.",['Computer Vision and Pattern Recognition']
Online Forecasting Matrix Factorization,"In this paper the problem of forecasting high dimensional time series is
considered. Such time series can be modeled as matrices where each column
denotes a measurement. In addition, when missing values are present, low rank
matrix factorization approaches are suitable for predicting future values. This
paper formally defines and analyzes the forecasting problem in the online
setting, i.e. where the data arrives as a stream and only a single pass is
allowed. We present and analyze novel matrix factorization techniques which can
learn low-dimensional embeddings effectively in an online manner. Based on
these embeddings a recursive minimum mean square error estimator is derived,
which learns an autoregressive model on them. Experiments with two real
datasets with tens of millions of measurements show the benefits of the
proposed approach.",['Machine Learning']
Non-Recursive Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) are powerful models for node
representation learning tasks. However, the node representation in existing GCN
models is usually generated by performing recursive neighborhood aggregation
across multiple graph convolutional layers with certain sampling methods, which
may lead to redundant feature mixing, needless information loss, and extensive
computations. Therefore, in this paper, we propose a novel architecture named
Non-Recursive Graph Convolutional Network (NRGCN) to improve both the training
efficiency and the learning performance of GCNs in the context of node
classification. Specifically, NRGCN proposes to represent different hops of
neighbors for each node based on inner-layer aggregation and layer-independent
sampling. In this way, each node can be directly represented by concatenating
the information extracted independently from each hop of its neighbors thereby
avoiding the recursive neighborhood expansion across layers. Moreover, the
layer-independent sampling and aggregation can be precomputed before the model
training, thus the training process can be accelerated considerably. Extensive
experiments on benchmark datasets verify that our NRGCN outperforms the
state-of-the-art GCN models, in terms of the node classification performance
and reliability.",['Machine Learning']
DDSL: Deep Differentiable Simplex Layer for Learning Geometric Signals,"We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for
geometric deep learning. The DDSL is a differentiable layer compatible with
deep neural networks for bridging simplex mesh-based geometry representations
(point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images
(e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to
perform differentiable, efficient, anti-aliased rasterization of simplex-based
signals. We present a complete theoretical framework for the process as well as
an efficient backpropagation algorithm. Compared to previous differentiable
renderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees
and dimensions. In particular, we explore its applications to 2D shapes and
illustrate two applications of this method: (1) mesh editing and optimization
guided by neural network outputs, and (2) using DDSL for a differentiable
rasterization loss to facilitate end-to-end training of polygon generators. We
are able to validate the effectiveness of gradient-based shape optimization
with the example of airfoil optimization, and using the differentiable
rasterization loss to facilitate end-to-end training, we surpass state of the
art for polygonal image segmentation given ground-truth bounding boxes.","['Computer Vision and Pattern Recognition', 'Computational Geometry']"
Active Learning for Node Classification: The Additional Learning Ability from Unlabelled Nodes,"Node classification on graph data is an important task on many practical
domains. However, it requires labels for training, which can be difficult or
expensive to obtain in practice. Given a limited labelling budget, active
learning aims to improve performance by carefully choosing which nodes to
label. Our empirical study shows that existing active learning methods for node
classification are considerably outperformed by a simple method which randomly
selects nodes to label and trains a linear classifier with labelled nodes and
unsupervised learning features. This indicates that existing methods do not
fully utilize the information present in unlabelled nodes as they only use
unlabelled nodes for label acquisition. In this paper, we utilize the
information in unlabelled nodes by using unsupervised learning features. We
propose a novel latent space clustering-based active learning method for node
classification (LSCALE). Specifically, to select nodes for labelling, our
method uses the K-Medoids clustering algorithm on a feature space based on the
dynamic combination of both unsupervised features and supervised features. In
addition, we design an incremental clustering module to avoid redundancy
between nodes selected at different steps. We conduct extensive experiments on
three public citation datasets and two co-authorship datasets, where our
proposed method LSCALE consistently and significantly outperforms the
state-of-the-art approaches by a large margin.","['Machine Learning', 'Social and Information Networks']"
In-sample Contrastive Learning and Consistent Attention for Weakly Supervised Object Localization,"Weakly supervised object localization (WSOL) aims to localize the target
object using only the image-level supervision. Recent methods encourage the
model to activate feature maps over the entire object by dropping the most
discriminative parts. However, they are likely to induce excessive extension to
the backgrounds which leads to over-estimated localization. In this paper, we
consider the background as an important cue that guides the feature activation
to cover the sophisticated object region and propose contrastive attention
loss. The loss promotes similarity between foreground and its dropped version,
and, dissimilarity between the dropped version and background. Furthermore, we
propose foreground consistency loss that penalizes earlier layers producing
noisy attention regarding the later layer as a reference to provide them with a
sense of backgroundness. It guides the early layers to activate on objects
rather than locally distinctive backgrounds so that their attentions to be
similar to the later layer. For better optimizing the above losses, we use the
non-local attention blocks to replace channel-pooled attention leading to
enhanced attention maps considering the spatial similarity. Last but not least,
we propose to drop background regions in addition to the most discriminative
region. Our method achieves state-of-theart performance on CUB-200-2011 and
ImageNet benchmark datasets regarding top-1 localization accuracy and
MaxBoxAccV2, and we provide detailed analysis on our individual components. The
code will be publicly available online for reproducibility.",['Computer Vision and Pattern Recognition']
Identity-aware Graph Neural Networks,"Message passing Graph Neural Networks (GNNs) provide a powerful modeling
framework for relational data. However, the expressive power of existing GNNs
is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test,
which means GNNs that are not able to predict node clustering coefficients and
shortest path distances, and cannot differentiate between different d-regular
graphs. Here we develop a class of message passing GNNs, named Identity-aware
Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL
test. ID-GNN offers a minimal but powerful solution to limitations of existing
GNNs. ID-GNN extends existing GNN architectures by inductively considering
nodes' identities during message passing. To embed a given node, ID-GNN first
extracts the ego network centered at the node, then conducts rounds of
heterogeneous message passing, where different sets of parameters are applied
to the center node than to other surrounding nodes in the ego network. We
further propose a simplified but faster version of ID-GNN that injects node
identity information as augmented node features. Altogether, both versions of
ID-GNN represent general extensions of message passing GNNs, where experiments
show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy
improvement on challenging node, edge, and graph property prediction tasks; 3%
accuracy improvement on node and graph classification benchmarks; and 15% ROC
AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs
demonstrate improved or comparable performance over other task-specific graph
networks.","['Machine Learning', 'Artificial Intelligence', 'Social and Information Networks']"
e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks,"Recently, there has been an increasing number of efforts to introduce models
capable of generating natural language explanations (NLEs) for their
predictions on vision-language (VL) tasks. Such models are appealing, because
they can provide human-friendly and comprehensive explanations. However, there
is a lack of comparison between existing methods, which is due to a lack of
re-usable evaluation frameworks and a scarcity of datasets. In this work, we
introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable
vision-language tasks that establishes a unified evaluation framework and
provides the first comprehensive comparison of existing approaches that
generate NLEs for VL tasks. It spans four models and three datasets and both
automatic metrics and human evaluation are used to assess model-generated
explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs
(over 430k instances). We also propose a new model that combines UNITER, which
learns joint embeddings of images and text, and GPT-2, a pre-trained language
model that is well-suited for text generation. It surpasses the previous state
of the art by a large margin across all datasets. Code and data are available
here: https://github.com/maximek3/e-ViL.","['Computer Vision and Pattern Recognition', 'Computation and Language', 'Machine Learning']"
Combiner: Full Attention Transformer with Sparse Computation Cost,"Transformers provide a class of expressive architectures that are extremely
effective for sequence modeling. However, the key limitation of transformers is
their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to
the sequence length in attention layers, which restricts application in
extremely long sequences. Most existing approaches leverage sparsity or
low-rank assumptions in the attention matrix to reduce cost, but sacrifice
expressiveness. Instead, we propose Combiner, which provides full attention
capability in each attention head while maintaining low computation and memory
complexity. The key idea is to treat the self-attention mechanism as a
conditional expectation over embeddings at each location, and approximate the
conditional distribution with a structured factorization. Each location can
attend to all other locations, either via direct attention, or through indirect
attention to abstractions, which are again conditional expectations of
embeddings from corresponding local regions. We show that most sparse attention
patterns used in existing sparse transformers are able to inspire the design of
such factorization for full attention, resulting in the same sub-quadratic cost
($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in
replacement for attention layers in existing transformers and can be easily
implemented in common frameworks. An experimental evaluation on both
autoregressive and bidirectional sequence tasks demonstrates the effectiveness
of this approach, yielding state-of-the-art results on several image and text
modeling tasks.","['Machine Learning', 'Computation and Language', 'Computer Vision and Pattern Recognition']"
Image Segmentation with Pseudo-marginal MCMC Sampling and Nonparametric Shape Priors,"In this paper, we propose an efficient pseudo-marginal Markov chain Monte
Carlo (MCMC) sampling approach to draw samples from posterior shape
distributions for image segmentation. The computation time of the proposed
approach is independent from the size of the training set used to learn the
shape prior distribution nonparametrically. Therefore, it scales well for very
large data sets. Our approach is able to characterize the posterior probability
density in the space of shapes through its samples, and to return multiple
solutions, potentially from different modes of a multimodal probability
density, which would be encountered, e.g., in segmenting objects from multiple
shape classes. Experimental results demonstrate the potential of the proposed
approach.",['Computer Vision and Pattern Recognition']
Attentive Weakly Supervised land cover mapping for object-based satellite image time series data with spatial interpretation,"Nowadays, modern Earth Observation systems continuously collect massive
amounts of satellite information. The unprecedented possibility to acquire high
resolution Satellite Image Time Series (SITS) data (series of images with high
revisit time period on the same geographical area) is opening new opportunities
to monitor the different aspects of the Earth Surface but, at the same time, it
is raising up new challenges in term of suitable methods to analyze and exploit
such huge amount of rich and complex image data. One of the main task
associated to SITS data analysis is related to land cover mapping where
satellite data are exploited via learning methods to recover the Earth Surface
status aka the corresponding land cover classes. Due to operational
constraints, the collected label information, on which machine learning
strategies are trained, is often limited in volume and obtained at coarse
granularity carrying out inexact and weak knowledge that can affect the whole
process. To cope with such issues, in the context of object-based SITS land
cover mapping, we propose a new deep learning framework, named TASSEL
(aTtentive weAkly Supervised Satellite image time sEries cLassifier), that is
able to intelligently exploit the weak supervision provided by the coarse
granularity labels. Furthermore, our framework also produces an additional
side-information that supports the model interpretability with the aim to make
the black box gray. Such side-information allows to associate spatial
interpretation to the model decision via visual inspection.",['Computer Vision and Pattern Recognition']
RankSRGAN: Super Resolution Generative Adversarial Networks with Learning to Rank,"Generative Adversarial Networks (GAN) have demonstrated the potential to
recover realistic details for single image super-resolution (SISR). To further
improve the visual quality of super-resolved results, PIRM2018-SR Challenge
employed perceptual metrics to assess the perceptual quality, such as PI, NIQE,
and Ma. However, existing methods cannot directly optimize these
indifferentiable perceptual metrics, which are shown to be highly correlated
with human ratings. To address the problem, we propose Super-Resolution
Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator
in the direction of different perceptual metrics. Specifically, we first train
a Ranker which can learn the behaviour of perceptual metrics and then introduce
a novel rank-content loss to optimize the perceptual quality. The most
appealing part is that the proposed method can combine the strengths of
different SR methods to generate better results. Furthermore, we extend our
method to multiple Rankers to provide multi-dimension constraints for the
generator. Extensive experiments show that RankSRGAN achieves visually pleasing
results and reaches state-of-the-art performance in perceptual metrics and
quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN",['Computer Vision and Pattern Recognition']
Learning to Resize Images for Computer Vision Tasks,"For all the ways convolutional neural nets have revolutionized computer
vision in recent years, one important aspect has received surprisingly little
attention: the effect of image size on the accuracy of tasks being trained for.
Typically, to be efficient, the input images are resized to a relatively small
spatial resolution (e.g. 224x224), and both training and inference are carried
out at this resolution. The actual mechanism for this re-scaling has been an
afterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic
are commonly used in most machine learning software frameworks. But do these
resizers limit the on task performance of the trained networks? The answer is
yes. Indeed, we show that the typical linear resizer can be replaced with
learned resizers that can substantially improve performance. Importantly, while
the classical resizers typically result in better perceptual quality of the
downscaled images, our proposed learned resizers do not necessarily give better
visual quality, but instead improve task performance. Our learned image resizer
is jointly trained with a baseline vision model. This learned CNN-based resizer
creates machine friendly visual manipulations that lead to a consistent
improvement of the end task metric over the baseline model. Specifically, here
we focus on the classification task with the ImageNet dataset, and experiment
with four different models to learn resizers adapted to each model. Moreover,
we show that the proposed resizer can also be useful for fine-tuning the
classification baselines for other vision tasks. To this end, we experiment
with three different baselines to develop image quality assessment (IQA) models
on the AVA dataset.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Multi-Task Learning of Generalizable Representations for Video Action Recognition,"In classic video action recognition, labels may not contain enough
information about the diverse video appearance and dynamics, thus, existing
models that are trained under the standard supervised learning paradigm may
extract less generalizable features. We evaluate these models under a
cross-dataset experiment setting, as the above label bias problem in video
analysis is even more prominent across different data sources. We find that
using the optical flows as model inputs harms the generalization ability of
most video recognition models.
  Based on these findings, we present a multi-task learning paradigm for video
classification. Our key idea is to avoid label bias and improve the
generalization ability by taking data as its own supervision or supervising
constraints on the data. First, we take the optical flows and the RGB frames by
taking them as auxiliary supervisions, and thus naming our model as Reversed
Two-Stream Networks (Rev2Net). Further, we collaborate the auxiliary flow
prediction task and the frame reconstruction task by introducing a new training
objective to Rev2Net, named Decoding Discrepancy Penalty (DDP), which
constraints the discrepancy of the multi-task features in a self-supervised
manner. Rev2Net is shown to be effective on the classic action recognition
task. It specifically shows a strong generalization ability in the
cross-dataset experiments.",['Computer Vision and Pattern Recognition']
A Two-stream Neural Network for Pose-based Hand Gesture Recognition,"Pose based hand gesture recognition has been widely studied in the recent
years. Compared with full body action recognition, hand gesture involves joints
that are more spatially closely distributed with stronger collaboration. This
nature requires a different approach from action recognition to capturing the
complex spatial features. Many gesture categories, such as ""Grab"" and ""Pinch"",
have very similar motion or temporal patterns posing a challenge on temporal
processing. To address these challenges, this paper proposes a two-stream
neural network with one stream being a self-attention based graph convolutional
network (SAGCN) extracting the short-term temporal information and hierarchical
spatial information, and the other being a residual-connection enhanced
bidirectional Independently Recurrent Neural Network (RBi-IndRNN) for
extracting long-term temporal information. The self-attention based graph
convolutional network has a dynamic self-attention mechanism to adaptively
exploit the relationships of all hand joints in addition to the fixed topology
and local feature extraction in the GCN. On the other hand, the
residual-connection enhanced Bi-IndRNN extends an IndRNN with the capability of
bidirectional processing for temporal modelling. The two streams are fused
together for recognition. The Dynamic Hand Gesture dataset and First-Person
Hand Action dataset are used to validate its effectiveness, and our method
achieves state-of-the-art performance.","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Multimedia']"
Multiview RGB-D Dataset for Object Instance Detection,"This paper presents a new multi-view RGB-D dataset of nine kitchen scenes,
each containing several objects in realistic cluttered environments including a
subset of objects from the BigBird dataset. The viewpoints of the scenes are
densely sampled and objects in the scenes are annotated with bounding boxes and
in the 3D point cloud. Also, an approach for detection and recognition is
presented, which is comprised of two parts: i) a new multi-view 3D proposal
generation method and ii) the development of several recognition baselines
using AlexNet to score our proposals, which is trained either on crops of the
dataset or on synthetically composited training images. Finally, we compare the
performance of the object proposals and a detection baseline to the Washington
RGB-D Scenes (WRGB-D) dataset and demonstrate that our Kitchen scenes dataset
is more challenging for object detection and recognition. The dataset is
available at: http://cs.gmu.edu/~robot/gmu-kitchens.html.","['Computer Vision and Pattern Recognition', 'Robotics']"
"Computational Attention System for Children, Adults and Elderly","The existing computational visual attention systems have focused on the
objective to basically simulate and understand the concept of visual attention
system in adults. Consequently, the impact of observer's age in scene viewing
behavior has rarely been considered. This study quantitatively analyzed the
age-related differences in gaze landings during scene viewing for three
different class of images: naturals, man-made, and fractals. Observer's of
different age-group have shown different scene viewing tendencies independent
to the class of the image viewed. Several interesting observations are drawn
from the results. First, gaze landings for man-made dataset showed that whereas
child observers focus more on the scene foreground, i.e., locations that are
near, elderly observers tend to explore the scene background, i.e., locations
farther in the scene. Considering this result a framework is proposed in this
paper to quantitatively measure the depth bias tendency across age groups.
Second, the quantitative analysis results showed that children exhibit the
lowest exploratory behavior level but the highest central bias tendency among
the age groups and across the different scene categories. Third,
inter-individual similarity metrics reveal that an adult had significantly
lower gaze consistency with children and elderly compared to other adults for
all the scene categories. Finally, these analysis results were consequently
leveraged to develop a more accurate age-adapted saliency model independent to
the image type. The prediction accuracy suggests that our model fits better to
the collected eye-gaze data of the observers belonging to different age groups
than the existing models do.","['Computer Vision and Pattern Recognition', 'Multimedia']"
Learned Equivariant Rendering without Transformation Supervision,"We propose a self-supervised framework to learn scene representations from
video that are automatically delineated into objects and background. Our method
relies on moving objects being equivariant with respect to their transformation
across frames and the background being constant. After training, we can
manipulate and render the scenes in real time to create unseen combinations of
objects, transformations, and backgrounds. We show results on moving MNIST with
backgrounds.",['Computer Vision and Pattern Recognition']
Representative Graph Neural Network,"Non-local operation is widely explored to model the long-range dependencies.
However, the redundant computation in this operation leads to a prohibitive
complexity. In this paper, we present a Representative Graph (RepGraph) layer
to dynamically sample a few representative features, which dramatically reduces
redundancy. Instead of propagating the messages from all positions, our
RepGraph layer computes the response of one node merely with a few
representative nodes. The locations of representative nodes come from a learned
spatial offset matrix. The RepGraph layer is flexible to integrate into many
visual architectures and combine with other operations. With the application of
semantic segmentation, without any bells and whistles, our RepGraph network can
compete or perform favourably against the state-of-the-art methods on three
challenging benchmarks: ADE20K, Cityscapes, and PASCAL-Context datasets. In the
task of object detection, our RepGraph layer can also improve the performance
on the COCO dataset compared to the non-local operation. Code is available at
https://git.io/RepGraph.",['Computer Vision and Pattern Recognition']
Ensembling with Deep Generative Views,"Recent generative models can synthesize ""views"" of artificial images that
mimic real-world variations, such as changes in color or pose, simply by
learning from unlabeled image collections. Here, we investigate whether such
views can be applied to real images to benefit downstream analysis tasks such
as image classification. Using a pretrained generator, we first find the latent
code corresponding to a given real input image. Applying perturbations to the
code creates natural variations of the image, which can then be ensembled
together at test-time. We use StyleGAN2 as the source of generative
augmentations and investigate this setup on classification tasks involving
facial attributes, cat faces, and cars. Critically, we find that several design
decisions are required towards making this process work; the perturbation
procedure, weighting between the augmentations and original image, and training
the classifier on synthesized images can all impact the result. Currently, we
find that while test-time ensembling with GAN-based augmentations can offer
some small improvements, the remaining bottlenecks are the efficiency and
accuracy of the GAN reconstructions, coupled with classifier sensitivities to
artifacts in GAN-generated images.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Pose Invariant Person Re-Identification using Robust Pose-transformation GAN,"The objective of person re-identification (re-ID) is to retrieve a person's
images from an image gallery, given a single instance of the person of
interest. Despite several advancements, learning discriminative
identity-sensitive and viewpoint invariant features for robust Person
Re-identification is a major challenge owing to the large pose variation of
humans. This paper proposes a re-ID pipeline that utilizes the image generation
capability of Generative Adversarial Networks combined with pose clustering and
feature fusion to achieve pose invariant feature learning. The objective is to
model a given person under different viewpoints and large pose changes and
extract the most discriminative features from all the appearances. The pose
transformational GAN (pt-GAN) module is trained to generate a person's image in
any given pose. In order to identify the most significant poses for
discriminative feature extraction, a Pose Clustering module is proposed. The
given instance of the person is modelled in varying poses and these features
are effectively combined through the Feature Fusion Network. The final re-ID
model consisting of these 3 sub-blocks, alleviates the pose dependence in
person re-ID. Also, The proposed model is robust to occlusion, scale, rotation
and illumination, providing a framework for viewpoint invariant feature
learning. The proposed method outperforms the state-of-the-art GAN based models
in 4 benchmark datasets. It also surpasses the state-of-the-art models that
report higher re-ID accuracy in terms of improvement over baseline.","['Computer Vision and Pattern Recognition', 'Artificial Intelligence', 'Machine Learning']"
A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View,"Accurate environment perception is essential for automated driving. When
using monocular cameras, the distance estimation of elements in the environment
poses a major challenge. Distances can be more easily estimated when the camera
perspective is transformed to a bird's eye view (BEV). For flat surfaces,
Inverse Perspective Mapping (IPM) can accurately transform images to a BEV.
Three-dimensional objects such as vehicles and vulnerable road users are
distorted by this transformation making it difficult to estimate their position
relative to the sensor. This paper describes a methodology to obtain a
corrected 360{\deg} BEV image given images from multiple vehicle-mounted
cameras. The corrected BEV image is segmented into semantic classes and
includes a prediction of occluded areas. The neural network approach does not
rely on manually labeled data, but is trained on a synthetic dataset in such a
way that it generalizes well to real-world data. By using semantically
segmented images as input, we reduce the reality gap between simulated and
real-world data and are able to show that our method can be successfully
applied in the real world. Extensive experiments conducted on the synthetic
data demonstrate the superiority of our approach compared to IPM. Source code
and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV","['Computer Vision and Pattern Recognition', 'Machine Learning']"
TransCrowd: Weakly-Supervised Crowd Counting with Transformer,"The mainstream crowd counting methods usually utilize the convolution neural
network (CNN) to regress a density map, requiring point-level annotations.
However, annotating each person with a point is an expensive and laborious
process. During the testing phase, the point-level annotations are not
considered to evaluate the counting accuracy, which means the point-level
annotations are redundant. Hence, it is desirable to develop weakly-supervised
counting methods that just rely on count level annotations, a more economical
way of labeling. Current weakly-supervised counting methods adopt the CNN to
regress a total count of the crowd by an image-to-count paradigm. However,
having limited receptive fields for context modeling is an intrinsic limitation
of these weakly-supervised CNN-based methods. These methods thus can not
achieve satisfactory performance, limited applications in the real-word. The
Transformer is a popular sequence-to-sequence prediction model in NLP, which
contains a global receptive field. In this paper, we propose TransCrowd, which
reformulates the weakly-supervised crowd counting problem from the perspective
of sequence-to-count based on Transformer. We observe that the proposed
TransCrowd can effectively extract the semantic crowd information by using the
self-attention mechanism of Transformer. To the best of our knowledge, this is
the first work to adopt a pure Transformer for crowd counting research.
Experiments on five benchmark datasets demonstrate that the proposed TransCrowd
achieves superior performance compared with all the weakly-supervised CNN-based
counting methods and gains highly competitive counting performance compared
with some popular fully-supervised counting methods. Code is available at
https://github.com/dk-liang/TransCrowd.",['Computer Vision and Pattern Recognition']
3D Cell Nuclei Segmentation with Balanced Graph Partitioning,"Cell nuclei segmentation is one of the most important tasks in the analysis
of biomedical images. With ever-growing sizes and amounts of three-dimensional
images to be processed, there is a need for better and faster segmentation
methods. Graph-based image segmentation has seen a rise in popularity in recent
years, but is seen as very costly with regard to computational demand. We
propose a new segmentation algorithm which overcomes these limitations. Our
method uses recursive balanced graph partitioning to segment foreground
components of a fast and efficient binarization. We construct a model for the
cell nuclei to guide the partitioning process. Our algorithm is compared to
other state-of-the-art segmentation algorithms in an experimental evaluation on
two sets of realistically simulated inputs. Our method is faster, has similar
or better quality and an acceptable memory overhead.","['Computer Vision and Pattern Recognition', 'Data Structures and Algorithms']"
Explicit topological priors for deep-learning based image segmentation using persistent homology,"We present a novel method to explicitly incorporate topological prior
knowledge into deep learning based segmentation, which is, to our knowledge,
the first work to do so. Our method uses the concept of persistent homology, a
tool from topological data analysis, to capture high-level topological
characteristics of segmentation results in a way which is differentiable with
respect to the pixelwise probability of being assigned to a given class. The
topological prior knowledge consists of the sequence of desired Betti numbers
of the segmentation. As a proof-of-concept we demonstrate our approach by
applying it to the problem of left-ventricle segmentation of cardiac MR images
of 500 subjects from the UK Biobank dataset, where we show that it improves
segmentation performance in terms of topological correctness without
sacrificing pixelwise accuracy.",['Computer Vision and Pattern Recognition']
Learning to Augment Expressions for Few-shot Fine-grained Facial Expression Recognition,"Affective computing and cognitive theory are widely used in modern
human-computer interaction scenarios. Human faces, as the most prominent and
easily accessible features, have attracted great attention from researchers.
Since humans have rich emotions and developed musculature, there exist a lot of
fine-grained expressions in real-world applications. However, it is extremely
time-consuming to collect and annotate a large number of facial images, of
which may even require psychologists to correctly categorize them. To the best
of our knowledge, the existing expression datasets are only limited to several
basic facial expressions, which are not sufficient to support our ambitions in
developing successful human-computer interaction systems. To this end, a novel
Fine-grained Facial Expression Database - F2ED is contributed in this paper,
and it includes more than 200k images with 54 facial expressions from 119
persons. Considering the phenomenon of uneven data distribution and lack of
samples is common in real-world scenarios, we further evaluate several tasks of
few-shot expression learning by virtue of our F2ED, which are to recognize the
facial expressions given only few training instances. These tasks mimic human
performance to learn robust and general representation from few examples. To
address such few-shot tasks, we propose a unified task-driven framework -
Compositional Generative Adversarial Network (Comp-GAN) learning to synthesize
facial images and thus augmenting the instances of few-shot expression classes.
Extensive experiments are conducted on F2ED and existing facial expression
datasets, i.e., JAFFE and FER2013, to validate the efficacy of our F2ED in
pre-training facial expression recognition network and the effectiveness of our
proposed approach Comp-GAN to improve the performance of few-shot recognition
tasks.",['Computer Vision and Pattern Recognition']
Pri3D: Can 3D Priors Help 2D Representation Learning?,"Recent advances in 3D perception have shown impressive progress in
understanding geometric structures of 3Dshapes and even scenes. Inspired by
these advances in geometric understanding, we aim to imbue image-based
perception with representations learned under geometric constraints. We
introduce an approach to learn view-invariant,geometry-aware representations
for network pre-training, based on multi-view RGB-D data, that can then be
effectively transferred to downstream 2D tasks. We propose to employ
contrastive learning under both multi-view im-age constraints and
image-geometry constraints to encode3D priors into learned 2D representations.
This results not only in improvement over 2D-only representation learning on
the image-based tasks of semantic segmentation, instance segmentation, and
object detection on real-world in-door datasets, but moreover, provides
significant improvement in the low data regime. We show a significant
improvement of 6.0% on semantic segmentation on full data as well as 11.9% on
20% data against baselines on ScanNet.",['Computer Vision and Pattern Recognition']
Co-occurrence Based Texture Synthesis,"As image generation techniques mature, there is a growing interest in
explainable representations that are easy to understand and intuitive to
manipulate. In this work, we turn to co-occurrence statistics, which have long
been used for texture analysis, to learn a controllable texture synthesis
model. We propose a fully convolutional generative adversarial network,
conditioned locally on co-occurrence statistics, to generate arbitrarily large
images while having local, interpretable control over the texture appearance.
To encourage fidelity to the input condition, we introduce a novel
differentiable co-occurrence loss that is integrated seamlessly into our
framework in an end-to-end fashion. We demonstrate that our solution offers a
stable, intuitive and interpretable latent representation for texture
synthesis, which can be used to generate a smooth texture morph between
different textures. We further show an interactive texture tool that allows a
user to adjust local characteristics of the synthesized texture image using the
co-occurrence values directly.",['Computer Vision and Pattern Recognition']
CaT: Weakly Supervised Object Detection with Category Transfer,"A large gap exists between fully-supervised object detection and
weakly-supervised object detection. To narrow this gap, some methods consider
knowledge transfer from additional fully-supervised dataset. But these methods
do not fully exploit discriminative category information in the
fully-supervised dataset, thus causing low mAP. To solve this issue, we propose
a novel category transfer framework for weakly supervised object detection. The
intuition is to fully leverage both visually-discriminative and
semantically-correlated category information in the fully-supervised dataset to
enhance the object-classification ability of a weakly-supervised detector. To
handle overlapping category transfer, we propose a double-supervision mean
teacher to gather common category information and bridge the domain gap between
two datasets. To handle non-overlapping category transfer, we propose a
semantic graph convolutional network to promote the aggregation of semantic
features between correlated categories. Experiments are conducted with Pascal
VOC 2007 as the target weakly-supervised dataset and COCO as the source
fully-supervised dataset. Our category transfer framework achieves 63.5% mAP
and 80.3% CorLoc with 5 overlapping categories between two datasets, which
outperforms the state-of-the-art methods. Codes are avaliable at
https://github.com/MediaBrain-SJTU/CaT.",['Computer Vision and Pattern Recognition']
Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities,"We present a new task of query auto-completion for estimating instance
probabilities. We complete a user query prefix conditioned upon an image. Given
the complete query, we fine tune a BERT embedding for estimating probabilities
of a broad set of instances. The resulting instance probabilities are used for
selection while being agnostic to the segmentation or attention mechanism. Our
results demonstrate that auto-completion using both language and vision
performs better than using only language, and that fine tuning a BERT embedding
allows to efficiently rank instances in the image. In the spirit of
reproducible research we make our data, models, and code available.","['Computer Vision and Pattern Recognition', 'Computation and Language', 'Machine Learning']"
Multilanguage Number Plate Detection using Convolutional Neural Networks,"Object Detection is a popular field of research for recent technologies. In
recent years, profound learning performance attracts the researchers to use it
in many applications. Number plate (NP) detection and classification is
analyzed over decades however, it needs approaches which are more precise and
state, language and design independent since cars are now moving from state to
another easily. In this paperwe suggest a new strategy to detect NP and
comprehend the nation, language and layout of NPs. YOLOv2 sensor with ResNet
attribute extractor heart is proposed for NP detection and a brand new
convolutional neural network architecture is suggested to classify NPs. The
detector achieves average precision of 99.57% and country, language and layout
classification precision of 99.33%. The results outperforms the majority of the
previous works and can move the area forward toward international NP detection
and recognition.",['Computer Vision and Pattern Recognition']
Fourier Series Expansion Based Filter Parametrization for Equivariant Convolutions,"It has been shown that equivariant convolution is very helpful for many types
of computer vision tasks. Recently, the 2D filter parametrization technique
plays an important role when designing equivariant convolutions. However, the
current filter parametrization method still has its evident drawbacks, where
the most critical one lies in the accuracy problem of filter representation.
Against this issue, in this paper we modify the classical Fourier series
expansion for 2D filters, and propose a new set of atomic basis functions for
filter parametrization. The proposed filter parametrization method not only
finely represents 2D filters with zero error when the filter is not rotated,
but also substantially alleviates the fence-effect-caused quality degradation
when the filter is rotated. Accordingly, we construct a new equivariant
convolution method based on the proposed filter parametrization method, named
F-Conv. We prove that the equivariance of the proposed F-Conv is exact in the
continuous domain, which becomes approximate only after discretization.
Extensive experiments show the superiority of the proposed method.
Particularly, we adopt rotation equivariant convolution methods to image
super-resolution task, and F-Conv evidently outperforms previous filter
parametrization based method in this task, reflecting its intrinsic capability
of faithfully preserving rotation symmetries in local image features.",['Computer Vision and Pattern Recognition']
Geography-Aware Self-Supervised Learning,"Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to remote sensing, where unlabeled data is often
abundant but labeled data is scarce. We first show that due to their different
characteristics, a non-trivial gap persists between contrastive and supervised
learning on standard benchmarks. To close the gap, we propose novel training
methods that exploit the spatiotemporal structure of remote sensing data. We
leverage spatially aligned images over time to construct temporal positive
pairs in contrastive learning and geo-location to design pre-text tasks. Our
experiments show that our proposed method closes the gap between contrastive
and supervised learning on image classification, object detection and semantic
segmentation for remote sensing and other geo-tagged image datasets.",['Computer Vision and Pattern Recognition']
Visual Rhythm Prediction with Feature-Aligning Network,"In this paper, we propose a data-driven visual rhythm prediction method,
which overcomes the previous works' deficiency that predictions are made
primarily by human-crafted hard rules. In our approach, we first extract
features including original frames and their residuals, optical flow, scene
change, and body pose. These visual features will be next taken into an
end-to-end neural network as inputs. Here we observe that there are some slight
misaligning between features over the timeline and assume that this is due to
the distinctions between how different features are computed. To solve this
problem, the extracted features are aligned by an elaborately designed layer,
which can also be applied to other models suffering from mismatched features,
and boost performance. Then these aligned features are fed into sequence
labeling layers implemented with BiLSTM and CRF to predict the onsets. Due to
the lack of existing public training and evaluation set, we experiment on a
dataset constructed by ourselves based on professionally edited Music Videos
(MVs), and the F1 score of our approach reaches 79.6.",['Computer Vision and Pattern Recognition']
Object Relational Graph with Teacher-Recommended Learning for Video Captioning,"Taking full advantage of the information from both vision and language is
critical for the video captioning task. Existing models lack adequate visual
representation due to the neglect of interaction between object, and sufficient
training for content-related words due to long-tailed problems. In this paper,
we propose a complete video captioning system including both a novel model and
an effective training strategy. Specifically, we propose an object relational
graph (ORG) based encoder, which captures more detailed interaction features to
enrich visual representation. Meanwhile, we design a teacher-recommended
learning (TRL) method to make full use of the successful external language
model (ELM) to integrate the abundant linguistic knowledge into the caption
model. The ELM generates more semantically similar word proposals which extend
the ground-truth words used for training to deal with the long-tailed problem.
Experimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the
proposed ORG-TRL system achieves state-of-the-art performance. Extensive
ablation studies and visualizations illustrate the effectiveness of our system.","['Computer Vision and Pattern Recognition', 'Computation and Language']"
Learning Graph Representations with Embedding Propagation,"We propose Embedding Propagation (EP), an unsupervised learning framework for
graph-structured data. EP learns vector representations of graphs by passing
two types of messages between neighboring nodes. Forward messages consist of
label representations such as representations of words and other attributes
associated with the nodes. Backward messages consist of gradients that result
from aggregating the label representations and applying a reconstruction loss.
Node representations are finally computed from the representation of their
labels. With significantly fewer parameters and hyperparameters an instance of
EP is competitive with and often outperforms state of the art unsupervised and
semi-supervised learning methods on a range of benchmark data sets.",['Machine Learning']
Causally-motivated Shortcut Removal Using Auxiliary Labels,"Robustness to certain forms of distribution shift is a key concern in many ML
applications. Often, robustness can be formulated as enforcing invariances to
particular interventions on the data generating process. Here, we study a
flexible, causally-motivated approach to enforcing such invariances, paying
special attention to shortcut learning, where a robust predictor can achieve
optimal i.i.d generalization in principle, but instead it relies on spurious
correlations or shortcuts in practice. Our approach uses auxiliary labels,
typically available at training time, to enforce conditional independences
between the latent factors that determine these labels. We show both
theoretically and empirically that causally-motivated regularization schemes
(a) lead to more robust estimators that generalize well under distribution
shift, and (b) have better finite sample efficiency compared to usual
regularization schemes, even in the absence of distribution shifts. Our
analysis highlights important theoretical properties of training techniques
commonly used in causal inference, fairness, and disentanglement literature.",['Machine Learning']
Channel Scaling: A Scale-and-Select Approach for Transfer Learning,"Transfer learning with pre-trained neural networks is a common strategy for
training classifiers in medical image analysis. Without proper channel
selections, this often results in unnecessarily large models that hinder
deployment and explainability. In this paper, we propose a novel approach to
efficiently build small and well performing networks by introducing the
channel-scaling layers. A channel-scaling layer is attached to each frozen
convolutional layer, with the trainable scaling weights inferring the
importance of the corresponding feature channels. Unlike the fine-tuning
approaches, we maintain the weights of the original channels and large datasets
are not required. By imposing L1 regularization and thresholding on the scaling
weights, this framework iteratively removes unnecessary feature channels from a
pre-trained model. Using an ImageNet pre-trained VGG16 model, we demonstrate
the capabilities of the proposed framework on classifying opacity from chest
X-ray images. The results show that we can reduce the number of parameters by
95% while delivering a superior performance.",['Computer Vision and Pattern Recognition']
Barycenters of Natural Images -- Constrained Wasserstein Barycenters for Image Morphing,"Image interpolation, or image morphing, refers to a visual transition between
two (or more) input images. For such a transition to look visually appealing,
its desirable properties are (i) to be smooth; (ii) to apply the minimal
required change in the image; and (iii) to seem ""real"", avoiding unnatural
artifacts in each image in the transition. To obtain a smooth and
straightforward transition, one may adopt the well-known Wasserstein Barycenter
Problem (WBP). While this approach guarantees minimal changes under the
Wasserstein metric, the resulting images might seem unnatural. In this work, we
propose a novel approach for image morphing that possesses all three desired
properties. To this end, we define a constrained variant of the WBP that
enforces the intermediate images to satisfy an image prior. We describe an
algorithm that solves this problem and demonstrate it using the sparse prior
and generative adversarial networks.",['Computer Vision and Pattern Recognition']
Scalable Scene Flow from Point Clouds in the Real World,"Autonomous vehicles operate in highly dynamic environments necessitating an
accurate assessment of which aspects of a scene are moving and where they are
moving to. A popular approach to 3D motion estimation, termed scene flow, is to
employ 3D point cloud data from consecutive LiDAR scans, although such
approaches have been limited by the small size of real-world, annotated LiDAR
data. In this work, we introduce a new large-scale dataset for scene flow
estimation derived from corresponding tracked 3D objects, which is
$\sim$1,000$\times$ larger than previous real-world datasets in terms of the
number of annotated frames. We demonstrate how previous works were bounded
based on the amount of real LiDAR data available, suggesting that larger
datasets are required to achieve state-of-the-art predictive performance.
Furthermore, we show how previous heuristics for operating on point clouds such
as down-sampling heavily degrade performance, motivating a new class of models
that are tractable on the full point cloud. To address this issue, we introduce
the FastFlow3D architecture which provides real time inference on the full
point cloud. Additionally, we design human-interpretable metrics that better
capture real world aspects by accounting for ego-motion and providing
breakdowns per object type. We hope that this dataset may provide new
opportunities for developing real world scene flow systems.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Explanation based Handwriting Verification,"Deep learning system have drawback that their output is not accompanied with
ex-planation. In a domain such as forensic handwriting verification it is
essential to provideexplanation to jurors. The goal of handwriting verification
is to find a measure of confi-dence whether the given handwritten samples are
written by the same or different writer.We propose a method to generate
explanations for the confidence provided by convolu-tional neural network (CNN)
which maps the input image to 15 annotations (features)provided by experts. Our
system comprises of: (1) Feature learning network (FLN),a differentiable
system, (2) Inference module for providing explanations. Furthermore,inference
module provides two types of explanations: (a) Based on cosine
similaritybetween categorical probabilities of each feature, (b) Based on
Log-Likelihood Ratio(LLR) using directed probabilistic graphical model. We
perform experiments using acombination of feature learning network (FLN) and
each inference module. We evaluateour system using XAI-AND dataset, containing
13700 handwritten samples and 15 cor-responding expert examined features for
each sample. The dataset is released for publicuse and the methods can be
extended to provide explanations on other verification taskslike face
verification and bio-medical comparison. This dataset can serve as the basis
and benchmark for future research in explanation based handwriting
verification. The code is available on github.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
TENT: Tensorized Encoder Transformer for Temperature Forecasting,"Reliable weather forecasting is of great importance in science, business and
society. The best performing data-driven models for weather prediction tasks
rely on recurrent or convolutional neural networks, where some of which
incorporate attention mechanisms. In this work, we introduce a new model based
on the Transformer architecture for weather forecasting. The proposed Tensorial
Encoder Transformer (TENT) model is equipped with tensorial attention and thus
it exploits the spatiotemporal structure of weather data by processing it in
multidimensional tensorial format. We show that compared to the encoder part of
the original transformer and 3D convolutional neural networks, the proposed
TENT model can better model the underlying complex pattern of weather data for
the studied temperature prediction task. Experiments on two real-life weather
datasets are performed. The datasets consist of historical measurements from
USA, Canada and European cities. The first dataset contains hourly measurements
of weather attributes for 30 cities in USA and Canada from October 2012 to
November 2017. The second dataset contains daily measurements of weather
attributes of 18 cities across Europe from May 2005 to April 2020. We use
attention scores calculated from our attention mechanism to shed light on the
decision-making process of our model and have insight knowledge on the most
important cities for the task.","['Machine Learning', 'Artificial Intelligence']"
Automating Vehicles by Deep Reinforcement Learning using Task Separation with Hill Climbing,"Within the context of autonomous driving a model-based reinforcement learning
algorithm is proposed for the design of neural network-parameterized
controllers. Classical model-based control methods, which include sampling- and
lattice-based algorithms and model predictive control, suffer from the
trade-off between model complexity and computational burden required for the
online solution of expensive optimization or search problems at every short
sampling time. To circumvent this trade-off, a 2-step procedure is motivated:
first learning of a controller during offline training based on an arbitrarily
complicated mathematical system model, before online fast feedforward
evaluation of the trained controller. The contribution of this paper is the
proposition of a simple gradient-free and model-based algorithm for deep
reinforcement learning using task separation with hill climbing (TSHC). In
particular, (i) simultaneous training on separate deterministic tasks with the
purpose of encoding many motion primitives in a neural network, and (ii) the
employment of maximally sparse rewards in combination with virtual velocity
constraints (VVCs) in setpoint proximity are advocated.","['Machine Learning', 'Robotics']"
LiMIIRL: Lightweight Multiple-Intent Inverse Reinforcement Learning,"Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a
reward function ensemble to rationalize demonstrations of different but
unlabelled intents. Within the popular expectation maximization (EM) framework
for learning probabilistic MI-IRL models, we present a warm-start strategy
based on up-front clustering of the demonstrations in feature space. Our
theoretical analysis shows that this warm-start solution produces a
near-optimal reward ensemble, provided the behavior modes satisfy mild
separation conditions. We also propose a MI-IRL performance metric that
generalizes the popular Expected Value Difference measure to directly assesses
learned rewards against the ground-truth reward ensemble. Our metric elegantly
addresses the difficulty of pairing up learned and ground truth rewards via a
min-cost flow formulation, and is efficiently computable. We also develop a
MI-IRL benchmark problem that allows for more comprehensive algorithmic
evaluations. On this problem, we find our MI-IRL warm-start strategy helps
avoid poor quality local minima reward ensembles, resulting in a significant
improvement in behavior clustering. Our extensive sensitivity analysis
demonstrates that the quality of the learned reward ensembles is improved under
various settings, including cases where our theoretical assumptions do not
necessarily hold. Finally, we demonstrate the effectiveness of our methods by
discovering distinct driving styles in a large real-world dataset of driver GPS
trajectories.","['Machine Learning', 'Artificial Intelligence', 'Robotics']"
Zero-Shot Learning via Class-Conditioned Deep Generative Models,"We present a deep generative model for learning to predict classes not seen
at training time. Unlike most existing methods for this problem, that represent
each class as a point (via a semantic embedding), we represent each seen/unseen
class using a class-specific latent-space distribution, conditioned on class
attributes. We use these latent-space distributions as a prior for a supervised
variational autoencoder (VAE), which also facilitates learning highly
discriminative feature representations for the inputs. The entire framework is
learned end-to-end using only the seen-class training data. The model infers
corresponding attributes of a test image by maximizing the VAE lower bound; the
inferred attributes may be linked to labels not seen when training. We further
extend our model to a (1) semi-supervised/transductive setting by leveraging
unlabeled unseen-class data via an unsupervised learning module, and (2)
few-shot learning where we also have a small number of labeled inputs from the
unseen classes. We compare our model with several state-of-the-art methods
through a comprehensive set of experiments on a variety of benchmark data sets.","['Machine Learning', 'Computer Vision and Pattern Recognition']"
Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers,"Recently, vision transformers and MLP-based models have been developed in
order to address some of the prevalent weaknesses in convolutional neural
networks. Due to the novelty of transformers being used in this domain along
with the self-attention mechanism, it remains unclear to what degree these
architectures are robust to corruptions. Despite some works proposing that data
augmentation remains essential for a model to be robust against corruptions, we
propose to explore the impact that the architecture has on corruption
robustness. We find that vision transformer architectures are inherently more
robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that
vision transformers with 5 times fewer parameters than a ResNet-50 have more
shape bias. Our code is available to reproduce.","['Computer Vision and Pattern Recognition', 'Machine Learning']"
Generating High Quality Visible Images from SAR Images Using CNNs,"We propose a novel approach for generating high quality visible-like images
from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative
Adversarial Network (GAN) architectures. The proposed approach is based on a
cascaded network of convolutional neural nets (CNNs) for despeckling and image
colorization. The cascaded structure results in faster convergence during
training and produces high quality visible images from the corresponding SAR
images. Experimental results on both simulated and real SAR images show that
the proposed method can produce visible-like images better compared to the
recent state-of-the-art deep learning-based methods.",['Computer Vision and Pattern Recognition']
Look Before you Speak: Visually Contextualized Utterances,"While most conversational AI systems focus on textual dialogue only,
conditioning utterances on visual context (when it's available) can lead to
more realistic conversations. Unfortunately, a major challenge for
incorporating visual context into conversational dialogue is the lack of
large-scale labeled datasets. We provide a solution in the form of a new
visually conditioned Future Utterance Prediction task. Our task involves
predicting the next utterance in a video, using both visual frames and
transcribed speech as context. By exploiting the large number of instructional
videos online, we train a model to solve this task at scale, without the need
for manual annotations. Leveraging recent advances in multimodal learning, our
model consists of a novel co-attentional multimodal video transformer, and when
trained on both textual and visual context, outperforms baselines that use
textual inputs alone. Further, we demonstrate that our model trained for this
task on unlabelled videos achieves state-of-the-art performance on a number of
downstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and
How2QA.","['Computer Vision and Pattern Recognition', 'Human-Computer Interaction']"
Learning Large DAGs by Combining Continuous Optimization and Feedback Arc Set Heuristics,"Bayesian networks represent relations between variables using a directed
acyclic graph (DAG). Learning the DAG is an NP-hard problem and exact learning
algorithms are feasible only for small sets of variables. We propose two
scalable heuristics for learning DAGs in the linear structural equation case.
Our methods learn the DAG by alternating between unconstrained gradient
descent-based step to optimize an objective function and solving a maximum
acyclic subgraph problem to enforce acyclicity. Thanks to this decoupling, our
methods scale up beyond thousands of variables.",['Machine Learning']
Action-Based Representation Learning for Autonomous Driving,"Human drivers produce a vast amount of data which could, in principle, be
used to improve autonomous driving systems. Unfortunately, seemingly
straightforward approaches for creating end-to-end driving models that map
sensor data directly into driving actions are problematic in terms of
interpretability, and typically have significant difficulty dealing with
spurious correlations. Alternatively, we propose to use this kind of
action-based driving data for learning representations. Our experiments show
that an affordance-based driving model pre-trained with this approach can
leverage a relatively small amount of weakly annotated imagery and outperform
pure end-to-end driving models, while being more interpretable. Further, we
demonstrate how this strategy outperforms previous methods based on learning
inverse dynamics models as well as other methods based on heavy human
supervision (ImageNet).","['Computer Vision and Pattern Recognition', 'Machine Learning', 'Robotics']"
Web-Scale Training for Face Identification,"Scaling machine learning methods to very large datasets has attracted
considerable attention in recent years, thanks to easy access to ubiquitous
sensing and data from the web. We study face recognition and show that three
distinct properties have surprising effects on the transferability of deep
convolutional networks (CNN): (1) The bottleneck of the network serves as an
important transfer learning regularizer, and (2) in contrast to the common
wisdom, performance saturation may exist in CNN's (as the number of training
samples grows); we propose a solution for alleviating this by replacing the
naive random subsampling of the training set with a bootstrapping process.
Moreover, (3) we find a link between the representation norm and the ability to
discriminate in a target domain, which sheds lights on how such networks
represent faces. Based on these discoveries, we are able to improve face
recognition accuracy on the widely used LFW benchmark, both in the verification
(1:1) and identification (1:N) protocols, and directly compare, for the first
time, with the state of the art Commercially-Off-The-Shelf system and show a
sizable leap in performance.",['Computer Vision and Pattern Recognition']
A Transfer Learning-based State of Charge Estimation for Lithium-Ion Battery at Varying Ambient Temperatures,"Accurate and reliable state of charge (SoC) estimation becomes increasingly
important to provide a stable and efficient environment for Lithium-ion
batteries (LiBs) powered devices. Most data-driven SoC models are built for a
fixed ambient temperature, which neglect the high sensitivity of LiBs to
temperature and may cause severe prediction errors. Nevertheless, a systematic
evaluation of the impact of temperature on SoC estimation and ways for a prompt
adjustment of the estimation model to new temperatures using limited data have
been hardly discussed. To solve these challenges, a novel SoC estimation method
is proposed by exploiting temporal dynamics of measurements and transferring
consistent estimation ability among different temperatures. First, temporal
dynamics, which is presented by correlations between the past fluctuation and
the future motion, is extracted using canonical variate analysis. Next, two
models, including a reference SoC estimation model and an estimation ability
monitoring model, are developed with temporal dynamics. The monitoring model
provides a path to quantitatively evaluate the influences of temperature on SoC
estimation ability. After that, once the inability of the reference SoC
estimation model is detected, consistent temporal dynamics between temperatures
are selected for transfer learning. Finally, the efficacy of the proposed
method is verified through a benchmark. Our proposed method not only reduces
prediction errors at fixed temperatures (e.g., reduced by 24.35% at -20{\deg}C,
49.82% at 25{\deg}C) but also improves prediction accuracies at new
temperatures.","['Machine Learning', 'Artificial Intelligence']"
"Bottom-up Attention, Models of","In this review, we examine the recent progress in saliency prediction and
proposed several avenues for future research. In spite of tremendous efforts
and huge progress, there is still room for improvement in terms finer-grained
analysis of deep saliency models, evaluation measures, datasets, annotation
methods, cognitive studies, and new applications. This chapter will appear in
Encyclopedia of Computational Neuroscience.",['Computer Vision and Pattern Recognition']
Deeply Aligned Adaptation for Cross-domain Object Detection,"Cross-domain object detection has recently attracted more and more attention
for real-world applications, since it helps build robust detectors adapting
well to new environments. In this work, we propose an end-to-end solution based
on Faster R-CNN, where ground-truth annotations are available for source images
(e.g., cartoon) but not for target ones (e.g., watercolor) during training.
Motivated by the observation that the transferabilities of different neural
network layers differ from each other, we propose to apply a number of domain
alignment strategies to different layers of Faster R-CNN, where the alignment
strength is gradually reduced from low to higher layers. Moreover, after
obtaining region proposals in our network, we develop a foreground-background
aware alignment module to further reduce the domain mismatch by separately
aligning features of the foreground and background regions from the source and
target domains. Extensive experiments on benchmark datasets demonstrate the
effectiveness of our proposed approach.",['Computer Vision and Pattern Recognition']
Factors in Finetuning Deep Model for object detection,"Finetuning from a pretrained deep model is found to yield state-of-the-art
performance for many vision tasks. This paper investigates many factors that
influence the performance in finetuning for object detection. There is a
long-tailed distribution of sample numbers for classes in object detection. Our
analysis and empirical results show that classes with more samples have higher
impact on the feature learning. And it is better to make the sample number more
uniform across classes. Generic object detection can be considered as multiple
equally important tasks. Detection of each class is a task. These classes/tasks
have their individuality in discriminative visual appearance representation.
Taking this individuality into account, we cluster objects into visually
similar class groups and learn deep representations for these groups
separately. A hierarchical feature learning scheme is proposed. In this scheme,
the knowledge from the group with large number of classes is transferred for
learning features in its sub-groups. Finetuned on the GoogLeNet model,
experimental results show 4.7% absolute mAP improvement of our approach on the
ImageNet object detection dataset without increasing much computational cost at
the testing stage.",['Computer Vision and Pattern Recognition']
EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks,"Recurrent neural networks (RNNs) are capable of modeling temporal
dependencies of complex sequential data. In general, current available
structures of RNNs tend to concentrate on controlling the contributions of
current and previous information. However, the exploration of different
importance levels of different elements within an input vector is always
ignored. We propose a simple yet effective Element-wise-Attention Gate
(EleAttG), which can be easily added to an RNN block (e.g. all RNN neurons in
an RNN layer), to empower the RNN neurons to have attentiveness capability. For
an RNN block, an EleAttG is used for adaptively modulating the input by
assigning different levels of importance, i.e., attention, to each
element/dimension of the input. We refer to an RNN block equipped with an
EleAttG as an EleAtt-RNN block. Instead of modulating the input as a whole, the
EleAttG modulates the input at fine granularity, i.e., element-wise, and the
modulation is content adaptive. The proposed EleAttG, as an additional
fundamental unit, is general and can be applied to any RNN structures, e.g.,
standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We
demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to
different tasks including the action recognition, from both skeleton-based data
and RGB videos, gesture recognition, and sequential MNIST classification.
Experiments show that adding attentiveness through EleAttGs to RNN blocks
significantly improves the power of RNNs.",['Computer Vision and Pattern Recognition']
Improving Long-Term Metrics in Recommendation Systems using Short-Horizon Reinforcement Learning,"We study session-based recommendation scenarios where we want to recommend
items to users during sequential interactions to improve their long-term
utility. Optimizing a long-term metric is challenging because the learning
signal (whether the recommendations achieved their desired goals) is delayed
and confounded by other user interactions with the system. Targeting
immediately measurable proxies such as clicks can lead to suboptimal
recommendations due to misalignment with the long-term metric. We develop a new
reinforcement learning algorithm called Short Horizon Policy Improvement (SHPI)
that approximates policy-induced drift in user behavior across sessions. SHPI
is a straightforward modification of episodic RL algorithms for session-based
recommendation, that additionally gives an appropriate termination bonus in
each session. Empirical results on four recommendation tasks show that SHPI can
outperform state-of-the-art recommendation techniques like matrix factorization
with offline proxy signals, bandits with myopic online proxies, and RL
baselines with limited amounts of user interaction.",['Machine Learning']
Survey: Transformer based Video-Language Pre-training,"Inspired by the success of transformer-based pre-training methods on natural
language tasks and further computer vision tasks, researchers have begun to
apply transformer to video processing. This survey aims to give a comprehensive
overview on transformer-based pre-training methods for Video-Language learning.
We first briefly introduce the transformer tructure as the background
knowledge, including attention mechanism, position encoding etc. We then
describe the typical paradigm of pre-training & fine-tuning on Video-Language
processing in terms of proxy tasks, downstream tasks and commonly used video
datasets. Next, we categorize transformer models into Single-Stream and
Multi-Stream structures, highlight their innovations and compare their
performances. Finally, we analyze and discuss the current challenges and
possible future research directions for Video-Language pre-training.",['Computer Vision and Pattern Recognition']
On Improving the Generalization of Face Recognition in the Presence of Occlusions,"In this paper, we address a key limitation of existing 2D face recognition
methods: robustness to occlusions. To accomplish this task, we systematically
analyzed the impact of facial attributes on the performance of a
state-of-the-art face recognition method and through extensive experimentation,
quantitatively analyzed the performance degradation under different types of
occlusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach
learned discriminative facial templates despite the presence of such
occlusions. First, an attention mechanism was proposed that extracted local
identity-related region. The local features were then aggregated with the
global representations to form a single template. Second, a simple, yet
effective, training strategy was introduced to balance the non-occluded and
occluded facial images. Extensive experiments demonstrated that OREO improved
the generalization ability of face recognition under occlusions by (10.17%) in
a single-image-based setting and outperformed the baseline by approximately
(2%) in terms of rank-1 accuracy in an image-set-based scenario.",['Computer Vision and Pattern Recognition']
Point Transformer for Shape Classification and Retrieval of 3D and ALS Roof PointClouds,"The success of deep learning methods led to significant breakthroughs in 3-D
point cloud processing tasks with applications in remote sensing. Existing
methods utilize convolutions that have some limitations, as they assume a
uniform input distribution and cannot learn long-range dependencies. Recent
works have shown that adding attention in conjunction with these methods
improves performance. This raises a question: can attention layers completely
replace convolutions? This paper proposes a fully attentional model - {\em
Point Transformer}, for deriving a rich point cloud representation. The model's
shape classification and retrieval performance are evaluated on a large-scale
urban dataset - RoofN3D and a standard benchmark dataset ModelNet40. Extensive
experiments are conducted to test the model's robustness to unseen point
corruptions for analyzing its effectiveness on real datasets. The proposed
method outperforms other state-of-the-art models in the RoofN3D dataset, gives
competitive results in the ModelNet40 benchmark, and showcases high robustness
to various unseen point corruptions. Furthermore, the model is highly memory
and space efficient when compared to other methods.",['Computer Vision and Pattern Recognition']
Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation,"Kinship verification is a long-standing research challenge in computer
vision. The visual differences presented to the face have a significant effect
on the recognition capabilities of the kinship systems. We argue that
aggregating multiple visual knowledge can better describe the characteristics
of the subject for precise kinship identification. Typically, the age-invariant
features can represent more natural facial details. Such age-related
transformations are essential for face recognition due to the biological
effects of aging. However, the existing methods mainly focus on employing the
single-view image features for kinship identification, while more meaningful
visual properties such as race and age are directly ignored in the feature
learning step. To this end, we propose a novel deep collaborative multi-modal
learning (DCML) to integrate the underlying information presented in facial
properties in an adaptive manner to strengthen the facial details for effective
unsupervised kinship verification. Specifically, we construct a well-designed
adaptive feature fusion mechanism, which can jointly leverage the complementary
properties from different visual perspectives to produce composite features and
draw greater attention to the most informative components of spatial feature
maps. Particularly, an adaptive weighting strategy is developed based on a
novel attention mechanism, which can enhance the dependencies between different
properties by decreasing the information redundancy in channels in a
self-adaptive manner. To validate the effectiveness of the proposed method,
extensive experimental evaluations conducted on four widely-used datasets show
that our DCML method is always superior to some state-of-the-art kinship
verification methods.",['Computer Vision and Pattern Recognition']
Deep Deformation Detail Synthesis for Thin Shell Models,"In physics-based cloth animation, rich folds and detailed wrinkles are
achieved at the cost of expensive computational resources and huge labor
tuning. Data-driven techniques make efforts to reduce the computation
significantly by a database. One type of methods relies on human poses to
synthesize fitted garments which cannot be applied to general cloth. Another
type of methods adds details to the coarse meshes without such restrictions.
However, existing works usually utilize coordinate-based representations which
cannot cope with large-scale deformation, and requires dense vertex
correspondences between coarse and fine meshes. Moreover, as such methods only
add details, they require coarse meshes to be close to fine meshes, which can
be either impossible, or require unrealistic constraints when generating fine
meshes. To address these challenges, we develop a temporally and spatially
as-consistent-as-possible deformation representation (named TS-ACAP) and a
DeformTransformer network to learn the mapping from low-resolution meshes to
detailed ones. This TS-ACAP representation is designed to ensure both spatial
and temporal consistency for sequential large-scale deformations from cloth
animations. With this representation, our DeformTransformer network first
utilizes two mesh-based encoders to extract the coarse and fine features,
respectively. To transduct the coarse features to the fine ones, we leverage
the Transformer network that consists of frame-level attention mechanisms to
ensure temporal coherence of the prediction. Experimental results show that our
method is able to produce reliable and realistic animations in various datasets
at high frame rates: 10 ~ 35 times faster than physics-based simulation, with
superior detail synthesis abilities than existing methods.","['Computer Vision and Pattern Recognition', 'Graphics']"
Combating Uncertainty with Novel Losses for Automatic Left Atrium Segmentation,"Segmenting left atrium in MR volume holds great potentials in promoting the
treatment of atrial fibrillation. However, the varying anatomies, artifacts and
low contrasts among tissues hinder the advance of both manual and automated
solutions. In this paper, we propose a fully-automated framework to segment
left atrium in gadolinium-enhanced MR volumes. The region of left atrium is
firstly automatically localized by a detection module. Our framework then
originates with a customized 3D deep neural network to fully explore the
spatial dependency in the region for segmentation. To alleviate the risk of low
training efficiency and potential overfitting, we enhance our deep network with
the transfer learning and deep supervision strategy. Main contribution of our
network design lies in the composite loss function to combat the boundary
ambiguity and hard examples. We firstly adopt the Overlap loss to encourage
network reduce the overlap between the foreground and background and thus
sharpen the predictions on boundary. We then propose a novel Focal Positive
loss to guide the learning of voxel-specific threshold and emphasize the
foreground to improve classification sensitivity. Further improvement is
obtained with an recursive training scheme. With ablation studies, all the
introduced modules prove to be effective. The proposed framework achieves an
average Dice of 92.24 in segmenting left atrium with pulmonary veins on 20
testing volumes.",['Computer Vision and Pattern Recognition']
Human Action Recognition with Multi-Laplacian Graph Convolutional Networks,"Convolutional neural networks are nowadays witnessing a major success in
different pattern recognition problems. These learning models were basically
designed to handle vectorial data such as images but their extension to
non-vectorial and semi-structured data (namely graphs with variable sizes,
topology, etc.) remains a major challenge, though a few interesting solutions
are currently emerging. In this paper, we introduce MLGCN; a novel spectral
Multi-Laplacian Graph Convolutional Network. The main contribution of this
method resides in a new design principle that learns graph-laplacians as convex
combinations of other elementary laplacians each one dedicated to a particular
topology of the input graphs. We also introduce a novel pooling operator, on
graphs, that proceeds in two steps: context-dependent node expansion is
achieved, followed by a global average pooling; the strength of this two-step
process resides in its ability to preserve the discrimination power of nodes
while achieving permutation invariance. Experiments conducted on SBU and
UCF-101 datasets, show the validity of our method for the challenging task of
action recognition.",['Computer Vision and Pattern Recognition']
Bayesian policy selection using active inference,"Learning to take actions based on observations is a core requirement for
artificial agents to be able to be successful and robust at their task.
Reinforcement Learning (RL) is a well-known technique for learning such
policies. However, current RL algorithms often have to deal with reward
shaping, have difficulties generalizing to other environments and are most
often sample inefficient. In this paper, we explore active inference and the
free energy principle, a normative theory from neuroscience that explains how
self-organizing biological systems operate by maintaining a model of the world
and casting action selection as an inference problem. We apply this concept to
a typical problem known to the RL community, the mountain car problem, and show
how active inference encompasses both RL and learning from demonstrations.","['Machine Learning', 'Artificial Intelligence', 'Neural and Evolutionary Computing']"
Transformation Consistent Self-ensembling Model for Semi-supervised Medical Image Segmentation,"Deep convolutional neural networks have achieved remarkable progress on a
variety of medical image computing tasks. A common problem when applying
supervised deep learning methods to medical images is the lack of labeled data,
which is very expensive and time-consuming to be collected. In this paper, we
present a novel semi-supervised method for medical image segmentation, where
the network is optimized by the weighted combination of a common supervised
loss for labeled inputs only and a regularization loss for both labeled and
unlabeled data. To utilize the unlabeled data, our method encourages the
consistent predictions of the network-in-training for the same input under
different regularizations. Aiming for the semi-supervised segmentation problem,
we enhance the effect of regularization for pixel-level predictions by
introducing a transformation, including rotation and flipping, consistent
scheme in our self-ensembling model. With the aim of semi-supervised
segmentation tasks, we introduce a transformation consistent strategy in our
self-ensembling model to enhance the regularization effect for pixel-level
predictions. We have extensively validated the proposed semi-supervised method
on three typical yet challenging medical image segmentation tasks: (i) skin
lesion segmentation from dermoscopy images on International Skin Imaging
Collaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus
images on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver
segmentation from volumetric CT scans on Liver Tumor Segmentation Challenge
(LiTS) dataset. Compared to the state-of-the-arts, our proposed method shows
superior segmentation performance on challenging 2D/3D medical images,
demonstrating the effectiveness of our semi-supervised method for medical image
segmentation.",['Computer Vision and Pattern Recognition']
Multi-Modal Prototype Learning for Interpretable Multivariable Time Series Classification,"Multivariable time series classification problems are increasing in
prevalence and complexity in a variety of domains, such as biology and finance.
While deep learning methods are an effective tool for these problems, they
often lack interpretability. In this work, we propose a novel modular prototype
learning framework for multivariable time series classification. In the first
stage of our framework, encoders extract features from each variable
independently. Prototype layers identify single-variable prototypes in the
resulting feature spaces. The next stage of our framework represents the
multivariable time series sample points in terms of their similarity to these
single-variable prototypes. This results in an inherently interpretable
representation of multivariable patterns, on which prototype learning is
applied to extract representative examples i.e. multivariable prototypes. Our
framework is thus able to explicitly identify both informative patterns in the
individual variables, as well as the relationships between the variables. We
validate our framework on a simulated dataset with embedded patterns, as well
as a real human activity recognition problem. Our framework attains comparable
or superior classification performance to existing time series classification
methods on these tasks. On the simulated dataset, we find that our model
returns interpretations consistent with the embedded patterns. Moreover, the
interpretations learned on the activity recognition dataset align with domain
knowledge.",['Machine Learning']
A Quorum Sensing Inspired Algorithm for Dynamic Clustering,"Quorum sensing is a decentralized biological process, through which a
community of cells with no global awareness coordinate their functional
behaviors based solely on cell-medium interactions and local decisions. This
paper draws inspirations from quorum sensing and colony competition to derive a
new algorithm for data clustering. The algorithm treats each data as a single
cell, and uses knowledge of local connectivity to cluster cells into multiple
colonies simultaneously. It simulates auto-inducers secretion in quorum sensing
to tune the influence radius for each cell. At the same time, sparsely
distributed core cells spread their influences to form colonies, and
interactions between colonies eventually determine each cell's identity. The
algorithm has the flexibility to analyze not only static but also time-varying
data, which surpasses the capacity of many existing algorithms. Its stability
and convergence properties are established. The algorithm is tested on several
applications, including both synthetic and real benchmarks data sets, alleles
clustering, community detection, image segmentation. In particular, the
algorithm's distinctive capability to deal with time-varying data allows us to
experiment it on novel applications such as robotic swarms grouping and
switching model identification. We believe that the algorithm's promising
performance would stimulate many more exciting applications.",['Machine Learning']
